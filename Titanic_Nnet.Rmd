---
title: "Titanic Nnet"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: readable
    highlight: tango
date: "2024-01-28"
navbar:
  title: "Simon Raymond"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
    - text: "Titanic XGBoosting"
      href: Titanic_XGBoosting.html
    - text: "Titanic Adaboost"
      href: Titanic_Adaboost.html
    - text: "Titanic Nnet"
      href: Titanic_Nnet.html
    - text: "Titanic_LM_CART_and_RF"
      href: Titanic_LM_CART_RF.html
    - text: "XGB_2"
      href: XGB_2.html
    - text: "Speeds"
      href: Speeds.html
name: SimonRaymond
---

# Data Prep

Download the Data
```{r}
library(randomForest)
library(readr)
library(dplyr)
library(ROCR)
library(nnet)
train_titanic <- read_csv("train.csv")
```

I will use the same data that i use in my other algorithms
```{r, echo = FALSE}
suppressPackageStartupMessages({
    library(plotly)
    library(readr)
    library(dplyr)
    library(forcats)
    library(stringr)
    library(tidyr)
    library(DataExplorer)
    library(randomForest)
    library(xgboost)
    library(shiny)
    library(GGally)
    library(ROCR)
    library(ggplot2)
    library(doParallel)
})

# Loading datasets
data <- read_csv("train.csv")

# Feature Engineering
data <- data %>%
  mutate(
    Ticket_Length = nchar(Ticket),
    Ticket_Is_Number = !grepl("\\D", Ticket),
    Title = str_extract(Name, "\\b[[:alpha:]]+\\."),
    Name_Length = nchar(Name),
    Cabin_Letter = ifelse(!is.na(Cabin), substr(Cabin, 1, 1), NA_character_),
    Has_Cabin = !is.na(Cabin)
  ) %>%
  add_count(Title) %>%
  mutate(
    Title = ifelse(n < 10, 'Other', as.character(Title)),
    Cabin_Letter = ifelse(Cabin_Letter %in% c('T', 'G'), 'Missing', as.character(Cabin_Letter)),
    Cabin_Letter = factor(Cabin_Letter)
  )

# Data Cleaning
data <- data %>%
  select(-Name, -Ticket, -Cabin) %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor) & !one_of("Age", "Embarked"), fct_explicit_na, na_level = "Missing")) %>%
  mutate(Embarked = replace(Embarked, is.na(Embarked), "S")) %>%
  mutate(Pclass = as.factor(Pclass))

# Imputing missing values in 'Age'
data_with_age <- data %>% filter(!is.na(Age))
data_missing_age <- data %>% filter(is.na(Age))

model_age <- randomForest(Age ~ . - PassengerId, data = data_with_age, na.action = na.omit)
predicted_ages <- predict(model_age, newdata = data_missing_age)
data_missing_age$Age <- predicted_ages

data <- rbind(data_with_age, data_missing_age) %>% arrange(PassengerId)

# Final preparation for the data
sdata <- data %>%
  mutate(across(where(is.numeric) & !all_of(c("Survived", "PassengerId")), scale)) %>%
  as_tibble()

# Prepare data for XGBOOST
sdata$Survived <- as.numeric(factor(sdata$Survived, labels = c(0, 1))) - 1

data <- sdata[, -1]
```

```{r}
str(data)
anyNA(data)
```

# nnet

This sets a wide grid

```{r}
size <- c(2:6)
decay <- c(seq(0.01, 0.25, 0.05))
maxit <- c(seq(100, 1500, 100))
grid <- expand.grid(size, decay, maxit)
```

```{r, warning = FALSE}
conf_lev <- .95
num_max <- 5 # Define number around the maximum
n <- log(1-conf_lev)/log(1-num_max/nrow(grid))
ind <- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE)
rgrid <- grid[ind, ]
```




This is not parrallel proccesed. Im just showing a 3 loop algorithm at its most basic level. in practive i would emply the xgb2 methodology

```{r}


n <- 10 # number of times we run the algorithm
v <- 50 # how many times we run each hyper parameter then take the mean of. for validation

# store out best values via validation scores
opt <- matrix(0, nrow = n, ncol = 5)
colnames(opt) <- c("size", "decay", "maxit", "AUC_val", "AUC_TEST")

for (j in 1:n){
  
  # put aside data for final test. creat md and test
ind <- unique(sample(nrow(data), nrow(data), replace = TRUE))
md <- data[ind, ]
test <- data[-ind, ]

auc_runs <- c() 
  
for (i in 1:nrow(rgrid)){
  #cat("loops: ", j, i, "\r")
  
   auc_tuning <- c()
    for (p in 1:v){
  # bootstrap from md to make a train and val set
 idx <- unique(sample(nrow(md), nrow(md), replace = TRUE))
  train <- md[idx,]
  val <- md[-idx, ]
  
# model on the train data
  model <- nnet(Survived ~ ., 
                data = train, 
                trace = FALSE, 
                act.fct = "logistic",
                size = rgrid[i, 1], 
                decay = rgrid[i, 2], 
                maxit = rgrid[i, 3]
                )
  # predict on the val data
  phat <- predict(model, val)
  
  # find the auc
  pred_rocr <- prediction(phat, val$Survived)
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc_tuning[p] <- auc_ROCR@y.values[[1]]
}
auc_runs[i] <- mean(auc_tuning) #take the mean of v runs for that one specific hyper parameter
}
# index the best hyper parameters 
BI <- which.max(auc_runs)
best_AUC <- auc_runs[BI]
best_params <- rgrid[BI, ]
  
# store the best hyper parames based on the mean aucs
  opt[j, 1] <- best_params[1, 1]
  opt[j, 2] <- best_params[1, 2]
  opt[j, 3] <- best_params[1, 3]
  opt[j, 4] <- best_AUC
 
# model with the md data
   model <- nnet(Survived ~ ., 
                data = md, 
                trace = FALSE,
                act.fct = "logistic",
                size = opt[j, 1], 
                decay = opt[j, 2], 
                maxit = opt[j, 3]
                )
  # predict the set aside test set
    phat_t <- predict(model, test)
  # get the test auc
  pred_rocr <- prediction(phat_t, test$Survived)
auc_ROCR <- performance(pred_rocr, measure = "auc")
auc_test <- auc_ROCR@y.values[[1]]
  

  # store the test auc
opt[j, 5] <- auc_test
}


```

Then we can go and choose a best hyperparameter set from here
```{r}
head(opt)
```



```{r, warning=FALSE}

auc <- opt[,5]

# plot auc and 95% CI
plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)

```