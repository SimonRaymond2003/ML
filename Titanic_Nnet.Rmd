---
title: "Titanic Nnet"
navbar:
  title: "Simon Raymond"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
    - text: "Titanic XGBoosting"
      href: Titanic_XGBoosting.html
    - text: "Titanic Adaboost"
      href: Titanic_Adaboost.html
    - text: "Titanic Nnet"
      href: Titanic_Nnet.html
    - text: "Titanic_LM_CART_and_RF"
      href: Titanic_LM_CART_RF.html
      name: SimonRaymond 
---

# Data Prep

Download the Data
```{r}
library(readr)
train_titanic <- read_csv("train.csv")
```

The columns i will use are y, Pclass, Sex, Age, SibSp, Parch, Fare, Emarked as they are the easiest to work with. other possibilities include using the tiltes for names 
```{r}
library(dplyr)

selected_cols <- c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")
data <- train_titanic[selected_cols]
data <- data %>% rename(y = Survived)
```

convert the characters to factors. convert our y variable to numeric for XGBoosting
```{r}
char_cols <- sapply(data, is.character)
data[, char_cols] <- lapply(data[, char_cols], factor)
data$y <- as.numeric(data$y)
```

we have NAs we need to deal with
```{r}
colSums(is.na(data))
```

Firsts i am going to predict the NAs for Age with a simple Random Forest model.
```{r}
na_index <- which(is.na(data$Age))#index the missing NAs
na_data <- data[na_index, ]#create a data frame of the NAs indexed
c_data <-data[-na_index, ]#take the complete data to c_data
na_data <- na_data[, -which(names(data) == "Age")] # take out the collum that only has NAs in it
```

 i will use na.roughfix() for the two missing Embarked values
```{r}
library(randomForest)
c_data <- na.roughfix(c_data)# there are two NAs in embarked
na_data <-  na.roughfix(na_data)
```

Predict our NAs
```{r}
library(randomForest)

model_rf <- randomForest(Age ~.,
                         ntree = 1200, 
                         data = c_data) 

pred_na <- predict(model_rf, na_data)
```

This binds our predictions to the corrosponding index number for each missing NA in "data"
```{r}
na_n_ind <- cbind(pred_na, na_index)

data$Age[na_n_ind[, 2]] <- na_n_ind[, 1]
```

finally just use na.roughfix() for the two embarked values since it is so little
```{r}
library(randomForest)

data <- na.roughfix(data)
colSums(is.na(data))
```

```{r}
str(data)
```


# nnet

This sets a wide grid
```{r}
size <- c(2:6)
decay <- c(seq(0.01, 0.25, 0.01))
maxit <- c(seq(100, 1500, 50))
grid <- expand.grid(size, decay, maxit)
```

```{r}
library(ROCR)
library(nnet)

n <- 50 # number of times we run the algorithm
s <- 500 # how many samples we take for the random grid/ to be validated
v <- 10 # how many times we run each hyper parameter then take the mean of. for validation

AUC <- c()


# store out best values via validation scores
opt <- matrix(0, nrow = n, ncol = 5)
colnames(opt) <- c("size", "decay", "maxit", "AUC_val", "AUC_TEST")



for (j in 1:n){
  
  # put aside data for final test. creat md and test
ind1 <- sample(nrow(data), nrow(data)*0.8, replace = FALSE)
md <- data[ind1, ]
test <- data[-ind1, ]

# make our random grid
ind <- sample(nrow(grid), s , replace = FALSE)
rgrid <- grid[ind, ]


auc_runs <- c() 
  
for (i in 1:nrow(rgrid)){
  #cat("loops: ", j, i, "\r")
  
   auc_tuning <- c()
    for (p in 1:v){
  # bootstrap from md to make a train and val set
 idx <- unique(sample(nrow(md), nrow(md), replace = TRUE))
  train <- md[idx,]
  val <- md[-idx, ]
# model on the train data
  model <- nnet(y ~ ., 
                data = train, 
                trace = FALSE, 
                act.fct = "logistic",
                size = rgrid[i, 1], 
                decay = rgrid[i, 2], 
                maxit = rgrid[i, 3]
                )
  # predict on the val data
  phat <- predict(model, val)
  
  # find the auc
  pred_rocr <- prediction(phat, val$y)
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc_tuning[p] <- auc_ROCR@y.values[[1]]
}
auc_runs[i] <- mean(auc_tuning) #take the mean of v runs for that one specific hyper parameter
}
# index the best hyper parameters 
BI <- which.max(auc_runs)
best_AUC <- auc_runs[BI]
best_params <- rgrid[BI, ]
  
# store the best hyper parames based on the mean aucs
  opt[j, 1] <- best_params[1, 1]
  opt[j, 2] <- best_params[1, 2]
  opt[j, 3] <- best_params[1, 3]
  opt[j, 4] <- best_AUC
 
# model with the md data
   model <- nnet(y ~ ., 
                data = md, 
                trace = FALSE,
                act.fct = "logistic",
                size = opt[j, 1], 
                decay = opt[j, 2], 
                maxit = opt[j, 3]
                )
  # predict the set aside test set
    phat_t <- predict(model, test)
  # get the test auc
  pred_rocr <- prediction(phat_t, test$y)
auc_ROCR <- performance(pred_rocr, measure = "auc")
auc_test <- auc_ROCR@y.values[[1]]
  

  # store the test auc
opt[j, 5] <- auc_test



  
}


```

index the best x percent of test aucs and there hyperparameters. we will store them in win
```{r}
th <- quantile(opt[ ,5], probs = 0.50)
ind_opt <- which(opt[ ,5] >= th)

win <- opt[ind_opt, ]
win
```

This code is using bootstrapping and runs the best hyperparamters that we picked before. it tests the selected hyper parameters n times. since we are not tuning in it we can use all of the data in it.
```{r}
library(nnet)
library(ROCR)

n <- 400
w <- nrow(win)

#collums are hyper param set
bs_results <- matrix(0, nrow = n, ncol = w)



for (i in 1:n) {
  for (j in 1:w){
    auc <- c()
  #cat("loops: ", i, j, "\r")
  
  # Initial Split
  ind <- unique(sample(nrow(data), nrow(data), replace = TRUE))
  md <- data[ind, ]
  test <- data[-ind, ]

  # build the model on model data
 model <- nnet(y ~ ., 
                data = md, 
                trace = FALSE, 
                act.fct = "logistic",
                size = win[j, 1], 
                decay = win[j, 2], 
                maxit = win[j, 3]
                )
  
  # Predicting on the test set
  phat <- predict(model, test)
  
  # Calculating the AUC
  pred_rocr <- prediction(phat, test$y)
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc <- auc_ROCR@y.values[[1]]
  
  # matrix with n rows and w collums. each collum is a hyper parameter
 bs_results[i, j] <- auc 
}
}
```

This code takes the colmeans of the matrix that gave our results. then we bind these results onto the win matrix for presentation. it also takes the index of the best run from before and saves the correlating hyperparameters into bh.
```{r}
bh_ind <- which.max(colMeans(bs_results))
bh <- win[bh_ind, ]
mean(bs_results[,bh_ind])
cm <- as.vector(colMeans(bs_results))
win <- cbind(win, av_ovr_n_runs=cm)
bh
```

This is the final run. it uses bootstrapping and runs n times. This is mostly to do a final proof and present the auc 
```{r, warning=FALSE}
library(gbm)
library(ROCR)

n <- 1000





 auc <- c()
for(i in 1:n){
    #cat("loops: ", i, "\r")
  
# bootstrapping 
  ind <- unique(sample(nrow(data), nrow(data), replace = TRUE))
  md <- data[ind, ]
  test <- data[-ind, ]
#
   model <- nnet(y ~ ., 
                data = md, 
                trace = FALSE, 
                act.fct = "logistic",
                size = bh[1], 
                decay = bh[2], 
                maxit = bh[3]
                )
  
  # Predicting on the test set
  phat <- predict(model, test)
  
  # Calculating the AUC
  pred_rocr <- prediction(phat, test$y)
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc[i] <- auc_ROCR@y.values[[1]]
  }

mean(auc)
sd(auc)
```

```{r, warning=FALSE}

# plot auc and 95% CI
plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)

```