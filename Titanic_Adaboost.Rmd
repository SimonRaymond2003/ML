---
title: "Titanic Adaboost"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: readable
    highlight: tango
date: "2024-01-28"
navbar:
  title: "Simon Raymond"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
    - text: "Titanic XGBoosting"
      href: Titanic_XGBoosting.html
    - text: "Titanic Adaboost"
      href: Titanic_Adaboost.html
    - text: "Titanic Nnet"
      href: Titanic_Nnet.html
    - text: "Titanic_LM_CART_and_RF"
      href: Titanic_LM_CART_RF.html
    - text: "XGB_2"
      href: XGB_2.html
    - text: "Speeds"
      href: Speeds.html
name: SimonRaymond
---

# Data Prep

Download the Data
```{r}
library(readr)
library(gbm)
train_titanic <- read_csv("train.csv")
```

I will use the same data that i use in my other algorithms
```{r, echo = FALSE}
suppressPackageStartupMessages({
    library(plotly)
    library(readr)
    library(dplyr)
    library(forcats)
    library(stringr)
    library(tidyr)
    library(DataExplorer)
    library(randomForest)
    library(xgboost)
    library(shiny)
    library(GGally)
    library(ROCR)
    library(ggplot2)
    library(doParallel)
})

# Loading datasets
data <- read_csv("train.csv")

# Feature Engineering
data <- data %>%
  mutate(
    Ticket_Length = nchar(Ticket),
    Ticket_Is_Number = !grepl("\\D", Ticket),
    Title = str_extract(Name, "\\b[[:alpha:]]+\\."),
    Name_Length = nchar(Name),
    Cabin_Letter = ifelse(!is.na(Cabin), substr(Cabin, 1, 1), NA_character_),
    Has_Cabin = !is.na(Cabin)
  ) %>%
  add_count(Title) %>%
  mutate(
    Title = ifelse(n < 10, 'Other', as.character(Title)),
    Cabin_Letter = ifelse(Cabin_Letter %in% c('T', 'G'), 'Missing', as.character(Cabin_Letter)),
    Cabin_Letter = factor(Cabin_Letter)
  )

# Data Cleaning
data <- data %>%
  select(-Name, -Ticket, -Cabin) %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor) & !one_of("Age", "Embarked"), fct_explicit_na, na_level = "Missing")) %>%
  mutate(Embarked = replace(Embarked, is.na(Embarked), "S")) %>%
  mutate(Pclass = as.factor(Pclass))

# Imputing missing values in 'Age'
data_with_age <- data %>% filter(!is.na(Age))
data_missing_age <- data %>% filter(is.na(Age))

model_age <- randomForest(Age ~ . - PassengerId, data = data_with_age, na.action = na.omit)
predicted_ages <- predict(model_age, newdata = data_missing_age)
data_missing_age$Age <- predicted_ages

data <- rbind(data_with_age, data_missing_age) %>% arrange(PassengerId)

# Final preparation for the data
sdata <- data %>%
  mutate(across(where(is.numeric) & !all_of(c("Survived", "PassengerId")), scale)) %>%
  as_tibble()

# Prepare data for XGBOOST
sdata$Survived <- as.numeric(factor(sdata$Survived, labels = c(0, 1))) - 1

data <- sdata[, -1]

data <- data %>%
  mutate(
    Ticket_Is_Number = as.factor(Ticket_Is_Number),
    Has_Cabin = as.factor(Has_Cabin)
  )

colnames(data)[1] <- c("y")
```

```{r}
str(data)
anyNA(data)
```


ADABOOST in gbm

This creates our Grid
```{r, warning=FALSE}
grid <- as.matrix(expand.grid(
  shrinkage <- seq(0.005, 0.25, 0.01), 
  n.trees <- seq(100, 2000, 400), 
  interaction.depth <- c(2:6)
  )
  )

conf_lev <- .95
num_max <- 5 # Define number around the maximum
n <- log(1-conf_lev)/log(1-num_max/nrow(grid))
ind <- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE)
rgrid <- grid[ind, ]
```

This code performs hyperparameter tuning for an Adaboost model in the GBM function/package. The number of iterations is controlled by the variable n. At each iteration, the code samples s hyperparameter combinations from a pre-defined grid. Then, the data is split into modeling and test data sets. the modeling data is boostrapped to make train and val data sets to to be used to select hyperparameters to be tested.

For each hyperparameter combination, the average area under the curve (AUC) is calculated across the v validation runs. The hyperparameter combination with the highest average AUC is selected as the best hyperparameters for that iteration. The selected hyperparameters and their corresponding AUC are stored in the opt matrix.

Finally, the code trains an Adaboostt model on the modeldata(both train and val) set using the best hyperparameters from each iteration. The AUC of that model is evaluated on a hold-out test set, and the results are stored in opt. 

Note that the hyperparameters being tuned are shrinkage, n.trees, and interaction.depth. The nthread parameter controls the number of threads to use during training.


```{r, warning=FALSE}
library(gbm)
library(ROCR)


n <- 10 # how many times we repeat the algorithm
v <- 50 # how many times we bootstrapped a sample for each hyperparameter set



opt <- matrix(0, nrow = n, ncol = 5, dimnames = list(NULL, c("shrinkage", " n.trees", "interaction.depth", "val_AUC", "Test_AUC")))

auc_t <- c()

for(j in 1:n){

  # put aside data for final test. creat md and test
ind <- unique(sample(nrow(data), nrow(data), replace = TRUE))
md <- data[ind, ]
test <- data[-ind, ]
  
    V_auc_mean <- c()
  for (i in 1:nrow(rgrid)){
    #cat("loops: ", j, i, "\r")
    
    auc_tuning <- c()
    for (p in 1:v){
        # bootstrap samples for train and val
  ind2 <- unique(sample(nrow(md), nrow(md), replace = TRUE))
  train <- md[ind2, ]
  val <- md[-ind2, ]
  
  # create a model off of train data
      model <- gbm(y ~ ., 
                   distribution = "adaboost", 
                   bag.fraction = 1, 
                   shrinkage = rgrid[i, 1], 
                   n.trees = rgrid[i, 2], 
                   interaction.depth = rgrid[i, 3],
                   verbose = FALSE,
                   data = train
                   )

      # Predicting on the val set
phat <- predict(model, n.trees = rgrid[i, 2], newdata = val, type = "response")

  # Calculating the AUC
  pred_rocr <- prediction(phat, val$y)
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc_tuning[p] <- auc_ROCR@y.values[[1]]
      
    }
   V_auc_mean[i] <- mean(auc_tuning) # for each hyperparameter we are taking the mean of their runs
  
     BI <- which.max(V_auc_mean) #best hyperparameter index
  best_AUC <- V_auc_mean[BI]
  best_params <- rgrid[BI, ]
  best_params <- as.matrix(best_params)
  best_params <- t(best_params)
  
  # store the hyperparameters and there average AUC
  opt[j, 1] <- best_params[1, 1]
  opt[j, 2] <- best_params[1, 2]
  opt[j, 3] <- best_params[1, 3]
  opt[j, 4] <- max(V_auc_mean)
  }
  
  # model for the final test witht he model data
   model <- gbm(y ~ ., 
                   distribution = "adaboost", 
                   bag.fraction = 1,
                   shrinkage = opt[j, 1], 
                   n.trees = opt[j, 2], 
                   interaction.depth = opt[j, 3],
                   n.cores = 3,
                   verbose = FALSE,
                   data = md
                   )

      # Predicting on the test set
 phat <- predict(model, newdata = test, n.trees = opt[j, 2], type = "response")

  # Calculating the AUC
  pred_rocr <- prediction(phat, test$y)
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc_t <- auc_ROCR@y.values[[1]]  
    
 # store the test score
  opt[j, 5] <- auc_t
}
head(opt)
```


```{r, warning=FALSE}
auc <- opt[, 5]
# plot auc and 95% CI
plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)

```

