---
title: "Titanic XGBoosting"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: readable
    highlight: tango
date: "2024-01-28"
navbar:
  title: "Simon Raymond"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
    - text: "Titanic XGBoosting"
      href: Titanic_XGBoosting.html
    - text: "Titanic Adaboost"
      href: Titanic_Adaboost.html
    - text: "Titanic Nnet"
      href: Titanic_Nnet.html
    - text: "Titanic_LM_CART_and_RF"
      href: Titanic_LM_CART_RF.html
    - text: "XGB_2"
      href: XGB_2.html
    - text: "Speeds"
      href: Speeds.html
name: SimonRaymond
---

```{r setup}
library(knitr)
opts_chunk$set(cache = TRUE, warning = FALSE)

```

Simon Raymond

## 1. Data Preparation and Setup

Initial data loading and basic transformations.

```{r, cache = TRUE, warning = FALSE}

suppressPackageStartupMessages({
    library(plotly)
    library(readr)
    library(dplyr)
    library(forcats)
    library(stringr)
    library(tidyr)
    library(DataExplorer)
    library(randomForest)
    library(xgboost)
    library(shiny)
    library(GGally)
    library(ROCR)
    library(ggplot2)
    library(doParallel)
})

```


```{r, cache = TRUE, warning = FALSE}
# Loading datasets
data <- read_csv("train.csv")
pd <- read_csv("test.csv")

# Initial data exploration
introduce(data)
data %>% plot_intro()
data %>% glimpse()
data %>% plot_missing()
data %>% profile_missing()
data %>% plot_density()


```

## 2. Feature Engineering

```{r, cache = TRUE, warning = FALSE}
# Extracting new features and handling missing values
data <- data %>%
  mutate(
    Ticket_Length = nchar(Ticket),
    Ticket_Is_Number = !grepl("\\D", Ticket),
    Title = str_extract(Name, "\\b[[:alpha:]]+\\."),
    Name_Length = nchar(Name),
    Cabin_Letter = ifelse(!is.na(Cabin), substr(Cabin, 1, 1), NA_character_),
    Has_Cabin = !is.na(Cabin)
  ) %>%
  add_count(Title) %>%
  mutate(
    Title = ifelse(n < 10, 'Other', as.character(Title)), # Adjust threshold if needed
    Cabin_Letter = ifelse(Cabin_Letter %in% c('T', 'G'), 'Missing', as.character(Cabin_Letter)),
    Cabin_Letter = factor(Cabin_Letter)
  )

```

## 3. Data Cleaning

Removing unnecessary columns, handling NAs, and imputing missing values for the Age column.

```{r, cache = TRUE, warning = FALSE}
# Removing unnecessary columns and initial handling of missing values
data <- data %>%
  select(-Name, -Ticket, -Cabin) %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor) & !one_of("Age", "Embarked"), fct_explicit_na, na_level = "Missing")) %>%
  mutate(Embarked = replace(Embarked, is.na(Embarked), "S")) %>% # from research
  mutate(Pclass = as.factor(Pclass))
```

Imputing missing values in the 'Age' column using Random Forest
```{r, cache = TRUE, warning = FALSE}
# Splitting the dataset
data_with_age <- data %>% filter(!is.na(Age))
data_missing_age <- data %>% filter(is.na(Age))

# Model training
model_age <- randomForest(Age ~ . - PassengerId, data = data_with_age, na.action = na.omit)

# Predicting and filling missing Ages
predicted_ages <- predict(model_age, newdata = data_missing_age)
data_missing_age$Age <- predicted_ages

# Combining datasets and reordering
data <- rbind(data_with_age, data_missing_age) %>% arrange(PassengerId)

```

## 4. Final preperation for the data 

Scale the numeric data
```{r, cache = TRUE, warning = FALSE}
# Scale numerical columns
sdata <- data %>%
  mutate(across(where(is.numeric) & !all_of(c("Survived", "PassengerId")), scale)) %>%
  as_tibble()
```

Get the data ready for XGBOOST
```{r, cache = TRUE, warning = FALSE}
# Convert 'Survived' to numeric for xgboost
sdata$Survived <- as.numeric(factor(sdata$Survived, labels = c(0, 1))) - 1

# Create model matrix for xgboost
xs <- model.matrix(~ . - 1 - Survived - PassengerId, data = sdata)
y <- sdata$Survived

```

## 5. Correlation Plots

```{r}
# Create dataset with numeric variables excluding 'PassengerId'
dn <- data %>%
  select(-PassengerId) %>%
  select_if(~is.numeric(.)) 

# Create dataset with factor variables excluding 'PassengerId'
df <- data %>%
  select(-PassengerId) %>%
  select_if(~is.factor(.)) %>%
  bind_cols(select(data, Survived))

total_levels <- sapply(df %>% select(-Survived), function(x) if(is.factor(x)) nlevels(x) else 0)
sorted_vars <- names(total_levels[order(total_levels, decreasing = TRUE)])
mid_point <- length(sorted_vars) / 2
group1_vars <- sorted_vars[1:ceiling(mid_point)]
group2_vars <- sorted_vars[(ceiling(mid_point) + 1):length(sorted_vars)]

group1_vars <- c(group1_vars, "Survived")
group2_vars <- c(group2_vars, "Survived")

# Create two new datasets based on these groups
df1 <- df %>% select(all_of(group1_vars))
df2 <- df %>% select(all_of(group2_vars))

plot_correlation(df1)
plot_correlation(df2)
plot_correlation(dn)
```


## 6. Feature Importance Analysis

In a randomforest model what is the auc and the most important predictors
```{r, cache = TRUE, warning = FALSE}
it <- 100 # Number of iterations
imp_df <- data.frame(matrix(0, nrow = it, ncol = 14))
colnames(imp_df) <- c("Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked", "Ticket_Length", 
                      "Ticket_Is_Number", "Title", "Name_Length", "n", "Cabin_Letter", "Has_Cabin")

auc <- numeric(it)  # ensure that 'it' is used here instead of 'n'
for (i in 1:it) {
    ind <- sample(nrow(data), nrow(data), replace = TRUE)
    train <- data[ind, ]
    test <- data[-ind, ]
    mod <- randomForest(Survived ~ . - PassengerId, data = train, ntree = 1200, family = "binomial")
    phat <- predict(mod, newdata = test, type = "response")
    pred_roc <- prediction(phat, as.numeric(test$Survived))
    auc[i] <- performance(pred_roc, "auc")@y.values[[1]]

    # Get the importance scores as a named vector
    imp_scores <- importance(mod)
    imp_scores_vector <- as.vector(imp_scores[,1])  # Assuming you want the first column of the importance matrix
    names(imp_scores_vector) <- rownames(imp_scores)

    # Assign the scores to the dataframe
    imp_df[i, names(imp_scores_vector)] <- imp_scores_vector
}

plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)
mean(auc)
sd(auc)
```

Rank the most important predictors
```{r}

col_means <- colMeans(imp_df)
mean_df <- data.frame(Feature = names(col_means), Mean = col_means)

# Plotting
ggplot(mean_df, aes(x = reorder(Feature, Mean), y = Mean)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Feature Importance", x = "Feature", y = "Mean Importance") +
  coord_flip()  # Flipping coordinates for better readability

```

Pclass
Description: Passenger class, a proxy for socio-economic status with 1 being the highest and 3 the lowest.
Origin: Extracted directly from the Titanic dataset. It's a crucial variable as passenger class was a significant factor in survival rates.

Sex
Description: Gender of the passengers, either male or female.
Origin: Directly from the Titanic dataset. Gender played a key role in survival, as women were often given priority for lifeboats.

Age
Description: Age of the passengers.
Origin: This is a primary variable from the Titanic dataset. Age, as a demographic factor, likely influenced survival chances.

SibSp
Description: Number of siblings or spouses aboard the Titanic.
Origin: Taken from the Titanic dataset, it indicates family size, which could influence survival decisions and opportunities.

Parch
Description: Number of parents or children aboard.
Origin: From the Titanic dataset, indicating if the passenger was traveling with family, especially with dependents.

Fare
Description: Passenger fare.
Origin: Represents the amount paid for the journey, sourced from the Titanic dataset. Higher fares might correlate with higher passenger classes.

Embarked
Description: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).
Origin: Part of the original dataset, potentially relevant as passengers' embarkation points might correlate with other socio-economic factors.

Ticket_Length
Description: Length of the ticket number as a numerical value.
Origin: Engineered feature, derived from the length of the ticket number string. Itâ€™s a proxy to differentiate ticket types.

Ticket_Is_Number
Description: Boolean indicating whether the ticket is purely numeric.
Origin: An engineered feature created to see if having a numeric ticket correlates with any survival pattern.

Title
Description: Title extracted from the passenger's name, such as Mr., Mrs., etc.
Origin: An engineered feature derived from passengers' names. It's useful in inferring social status, gender, and marital status.

Name_Length
Description: Length of the passenger's name.
Origin: This is an engineered feature, indicating possibly if longer names (which might include titles or honorifics) correlate with social status or survival rate.

n
Description: Frequency of each title in the dataset.
Origin: This engineered feature counts how common each title is, which can provide insights into the typical social status or role groups aboard the ship.

Cabin_Letter
Description: The first letter of the cabin number, which could indicate the deck or location on the ship.
Origin: Extracted and engineered from the cabin information in the dataset, giving an idea about the passenger's accommodation location which could influence survival likelihood.

Has_Cabin
Description: A Boolean indicating whether the passenger had a cabin on the Titanic.
Origin: An engineered feature from the cabin data, representing whether the passenger had a private cabin, potentially correlating with class and survival rate.

## 7. XGBoost Algorithm Function

### Test size

I want to quickly note that in my functions for my validation runs i use bootstrapping as such.
```{r}
# the length is just to show the % split that is being put into the index
length(unique(sample(100, 100, replace = TRUE)))
```

however for my Test set i am increasing the number of times a sample a row index so that my model data is larger. i want a range closer the 20-25 % because my data is so small.
```{r}
length(unique(sample(100, 133, replace = TRUE)))
```

### Main function

```{r}
run_xgb <- function(xs, y, grid, n, v) {
  
  # Set up parallel computing
  numCores <- detectCores() - 1
  cl <- makeCluster(numCores)
  registerDoParallel(cl)

  results <- foreach(i = 1:n, .combine='rbind', .packages=c('xgboost', 'ROCR')) %dopar% {
    tryCatch({
      # Bootstrap sampling
      ind <- unique(sample(nrow(xs), round((nrow(xs) + nrow(xs)/3)), replace = TRUE))
      md_x <- xs[ind, ]
      md_y <- y[ind]
      test_x <- xs[-ind, ]
      test_y <- y[-ind]

      auc <- numeric(nrow(grid))
      for (j in 1:nrow(grid)) {
          auc_100 <- numeric(v)
          for (k in 1:v) {
              # Nested validation loop
              ind2 <- unique(sample(nrow(md_x), nrow(md_x), replace = TRUE))
              train_x <- md_x[ind2, ]
              train_y <- md_y[ind2]
              val_x <- md_x[-ind2, ]  
              val_y <- md_y[-ind2]

              # Model training
              params <- list(
                  booster = "gbtree",
                  objective = "binary:logistic",
                  max_depth = grid[j, "max_depth"], 
                  eta = grid[j, "eta"],
                  subsample = grid[j, "subsample"],
                  colsample_bytree = grid[j, "colsample_bytree"],
                  gamma = grid[j, "gamma"],
                  min_child_weight = grid[j, "min_child_weight"],
                  alpha = grid[j, "alpha"],
                  lambda = grid[j, "lambda"]
              )
              xgb_train <- xgb.DMatrix(data = train_x, label = train_y) 
              xm <- xgb.train(params = params, data = xgb_train, nrounds = grid[j, "nrounds"], verbose = FALSE, nthread = 1)
              
              # AUC calculation
              phat <- predict(xm, xgb.DMatrix(data = val_x))
              pred_rocr <- prediction(phat, val_y)
              auc_100[k] <- performance(pred_rocr, "auc")@y.values[[1]]
          }
          auc[j] <- mean(auc_100)
      }

      # Get the best hyperparameters
      BI <- which.max(auc)
      params_best <- grid[BI, ]

      # Retrain and test
      params <- list(
          booster = "gbtree",
          objective = "binary:logistic",
          max_depth = grid[BI, "max_depth"], 
          eta = grid[BI, "eta"],
          subsample = grid[BI, "subsample"],
          colsample_bytree = grid[BI, "colsample_bytree"],
          gamma = grid[BI, "gamma"],
          min_child_weight = grid[BI, "min_child_weight"],
          alpha = grid[BI, "alpha"],
          lambda = grid[BI, "lambda"]
      )
      xgb_dm <- xgb.DMatrix(data = md_x, label = md_y)
      xgb_dt <- xgb.DMatrix(data = test_x, label = test_y)
      xmt <- xgb.train(params = params, data = xgb_dm, nrounds = grid[BI, "nrounds"], verbose = FALSE, nthread = 1)
      
      phat_t <- predict(xmt, xgb_dt)
      pred_rocr_t <- prediction(phat_t, test_y)
      auc_t <- performance(pred_rocr_t, "auc")@y.values[[1]]

      # Combine results
      c(params_best, auc[BI], auc_t, BI)
    }, error = function(e) {
      # Handle the error
      message("Error in iteration ", i, ": ", e$message)
      return(NA) # Return NA or any other value you deem appropriate
    })
  }

  # Stop the cluster
  stopCluster(cl)

  # Convert the results matrix to a tibble
  rt <- as_tibble(results, .name_repair = "universal")

  # Assign column names
  colnames(rt) <- c("eta", "max_depth", "min_child_weight", "subsample", "colsample_bytree", "lambda", "alpha", "gamma", "nrounds", "AUC_val_v_runs", "AUC_Test", "Rgrid_ind")

  # Unlist each column and return as a tibble
  aaa <- rt %>%
    mutate(
      eta = unlist(eta),
      max_depth = unlist(max_depth),
      min_child_weight = unlist(min_child_weight),
      subsample = unlist(subsample),
      colsample_bytree = unlist(colsample_bytree),
      lambda = unlist(lambda),
      alpha = unlist(alpha),
      gamma = unlist(gamma),
      nrounds = unlist(nrounds),
      AUC_val_v_runs = unlist(AUC_val_v_runs),
      AUC_Test = unlist(AUC_Test),
      Rgrid_ind = unlist(Rgrid_ind)
    )

  return(aaa)
}

```

### nrounds and eta function

This is gonna be a function to set eta and tune for nrounds. In this case i will use it at the beginning to show a idea of how fast the xgboost model is reaching a resonable rnounds range then again at the end to do a final tune for nrounds.

```{r}
opt_nrd4eta <- function(xs, y, params, o , grd) {
  # Assuming xgb.train and related functions are already available (via library(xgboost))
  library(xgboost)
  library(ROCR)
  
  # Detect the number of cores
  numCores <- detectCores() - 1
  
  # Initialize an AUC matrix
  aucm <- matrix(0, nrow = o, ncol = length(grd))
  
  start_time <- Sys.time()
  
  for (j in 1:o){
    # Creating a bootstrap sample
    ind <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
    
    dm <- xgb.DMatrix(data = xs[ind, ], label = y[ind])
    dv <- xgb.DMatrix(data = xs[-ind, ], label = y[-ind])
    
    auc <- c()
    for (i in 1:length(grd)){
      
      # Training the model on the bootstrap sample
      bsm <- xgb.train(params = params,
                       data = dm,
                       nrounds = grd[i],
                       verbose = FALSE,
                       nthread = numCores
                       )
      
      # Predict on the validation set and calculate AUC
      phat <- predict(bsm, dv, type = "prob")
      
      # Calculating the AUC
      pred_rocr <- prediction(phat, y[-ind])
      auc_ROCR <- performance(pred_rocr, measure = "auc")
      auc[i] <- auc_ROCR@y.values[[1]] 
    }
    aucm[j, ] <- auc
  }
  
  evalauc <- colMeans(aucm)
  
  # Plotting
  plot(grd, evalauc, type = "b", col = "blue", 
       xlab = "Number of Rounds", ylab = "AUC", 
       main = "AUC over o rounds vs Number of Rounds in XGBoost")
  
  best_nrounds <- grd[which.max(evalauc)]
  max_auc <- max(evalauc)
  
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  
  return(list(best_nrounds = best_nrounds, max_auc = max_auc, elapsed_time = elapsed_time))
}
```

### Kill rgrid Length Function

```{r}


kill_rg <- function(xs, y, rgrid, fv) {
  
  cl <- makeCluster(detectCores() - 1)
  registerDoParallel(cl)
  
  # Perform hyperparameter tuning
  auc <- foreach(j = 1:nrow(rgrid), .combine = c, .packages = c("xgboost", "ROCR")) %dopar% {
    vauc <- numeric(fv)
    
    for (i in 1:fv) {
      # Bootstrap sampling
      ind <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
      unique_ind <- unique(ind)
      dtrain_x <- xs[unique_ind, ]
      dtrain_y <- y[unique_ind]
      test_x <- xs[-unique_ind, ]
      test_y <- y[-unique_ind]
      
      # Model training with current set of hyperparameters
      params <- list(
        booster = "gbtree",
        objective = "binary:logistic",
        max_depth = rgrid[j, "max_depth"],
        eta = rgrid[j, "eta"],
        subsample = rgrid[j, "subsample"],
        colsample_bytree = rgrid[j, "colsample_bytree"],
        gamma = rgrid[j, "gamma"],
        min_child_weight = rgrid[j, "min_child_weight"],
        alpha = rgrid[j, "alpha"],
        lambda = rgrid[j, "lambda"]
      )
      
      xgb_train <- xgb.DMatrix(data = dtrain_x, label = dtrain_y)
      xm <- xgb.train(params = params, data = xgb_train, nrounds = rgrid[j, "nrounds"], verbose = FALSE, nthread = 1)
      
      # AUC calculation
      phat <- predict(xm, xgb.DMatrix(data = test_x))
      pred_rocr <- prediction(phat, test_y)
      vauc[i] <- performance(pred_rocr, "auc")@y.values[[1]]
    }
    
    mean(vauc)
  }
  
  # Stop the parallel backend
  stopCluster(cl)
  
  # Process AUC results to filter the top 50% performing hyperparameter sets
  sorted_indices <- order(auc, decreasing = TRUE)
  top_indices <- sorted_indices[1:(length(sorted_indices)/2)]
  kgrid <- rgrid[top_indices, ]
  
  # Return the filtered hyperparameter grid
  return(kgrid)
}

```

## 8. Run our model

### Get a general feel for nrounds value

This is the process we should be listening too. i will show why we dont use early stopping rounds with watchlist etc
```{r}

# Define XGBoost parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1 
)
grd <- seq(1, 200, by = 10)
o <- 100
rs <- opt_nrd4eta(xs = xs, y = y, params = params, o, grd)


print(rs)
```

We see that nrounds peaks very very fast. if we do xgboosting watchlistit very fast there aswell but the watchlist/early_stopping_rounds has a issue i will talk about.

I want to note that the internal xgboost watchlist early stopping rounds is very similar but not the same exactly.. it shows it its peak very fast

We must use my method due to the fact that in mine each point of the graph is 100 runs so it is more reliable. However i will print out the xgboost internal way of of seeing because it bring up a very important point

Now  we use early stopping rounds with bootstrapping
```{r}
params <- list(booster = "gbtree",
               objective = "binary:logistic",
               eta = 0.1)

max_rounds <- 1000
esr <- 10
n <- 1000

# Vector to store the best iteration for each bootstrap sample
bit <- c()
auc <- c()

for (i in 1:n){
  
    # Creating a bootstrap sample
    ind <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
    
    dm <- xgb.DMatrix(data = xs[ind, ], label = y[ind])
    dv <- xgb.DMatrix(data = xs[-ind, ], label = y[-ind])
    
    # Training the model on the bootstrap sample
    bsm <- xgb.train(params = params,
                     data = dm,
                     nrounds = max_rounds,
                     early_stopping_rounds = esr, 
                     watchlist = list(train = dm, val = dv),
                     eval_metric = "auc",
                     maximize = TRUE,
                     verbose = FALSE
                     )

    # Storing the best iteration
    bit[i] <- bsm$best_iteration
    
  # Predict on the validation set and calculate AUC
  phat <- predict(bsm, dv, type = "prob")

  # Calculating the AUC
  pred_rocr <- prediction(phat, y[-ind])
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc[i] <- auc_ROCR@y.values[[1]] 
}



frequency <- table(factor(bit))

# Creating a bar plot
barplot(frequency, main = "Frequency of Best Iterations", 
        xlab = "Best Iteration", ylab = "Frequency")


```

Looking just at watchlist early stopping rounds someone could assume that the best nrounds is 1. !THIS IS NOT THE CASE!

I dislike k fold cross validation as it give varying results but it also says that the best iterations are not the first few iterations 

The ranges seem to be from 40 - 200 acording the cv. but i dont trust that due to how varying the results are
```{r}

# Set parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1
)

num_rounds <- 1000
esr <- 50

# Create DMatrix
d <- xgb.DMatrix(data = xs, label = y)

# Perform cross-validation
cvr<- xgb.cv(params = params,
                     data = d,
                     nfold = 5,
                     nrounds = num_rounds,
                     early_stopping_rounds = esr,
                     maximize = TRUE,  
                     verbose = TRUE,
                     showsd = T,
                     stratified = T,
                     print.every.n = 10,
                     eval_metric = "auc"
                     )

# Get the average AUC across all folds
avg_auc <- max(cvr$evaluation_log$test_auc_mean)
cat("Average AUC across all folds: ", avg_auc, "\n")
cvr$best_iteration
```


I want to draw huge attention that this way is actually different then our way. This way takes the SINGLE best iteration from 1000 loops. it is not averaged out like in ours...

For example iteration 1 or 2 may have extreme volatility in auc (this is what we observe in this data).. because of this high volatility it will win its run more often then iteration 30. HOWEVER just because it wins its iteration more doesnt mean it is better. Iteration 30 will on average be better then iteration 2.

We can't just say this tho we have to test it....

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1 
)

# Define the range of grid values for number of rounds
grd <- c(1, 3, 30, 40, 100, 200)

# Set the number of iterations
o <- 100

# Call the function to optimize the number of rounds
result <- opt_nrd4eta(xs, y, params, o, grd)

```

While i dont want to beat a dead horse into the ground i will do one last test so show that the lower values (1-10) are not to be tuned on. 

This will set eta to 0.1 as the internal xgboost best_iteration would say is the best. we run it 100 times and look at the auc and how low it is.

```{r}
params <- list(booster = "gbtree",
               objective = "binary:logistic",
               eta = 0.1)

n <- 250

auc <- c()
for (i in 1:n){
  
    # Creating a bootstrap sample
    ind <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
    
    dm <- xgb.DMatrix(data = xs[ind, ], label = y[ind])
    dv <- xgb.DMatrix(data = xs[-ind, ], label = y[-ind])
    
    # Training the model on the bootstrap sample
    bsm <- xgb.train(params = params,
                     data = dm,
                     nrounds = 1
                     )

    
  phat <- predict(bsm, dv, type = "prob")

  pred_rocr <- prediction(phat, y[-ind])
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc[i] <- auc_ROCR@y.values[[1]] 
  
}

# plot auc and 95% CI of the runs
plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)

```
This supports exactly what we were saying. These low nrounds have a lower auc and higher variation when properly run

```{r}
sd(auc)
mean(auc)
```

Show nrounds 50 has better auc and lower sd
```{r}
params <- list(booster = "gbtree",
               objective = "binary:logistic",
               eta = 0.1)

n <- 250

auc <- c()
for (i in 1:n){
  
    # Creating a bootstrap sample
    ind <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
    
    dm <- xgb.DMatrix(data = xs[ind, ], label = y[ind])
    dv <- xgb.DMatrix(data = xs[-ind, ], label = y[-ind])
    
    # Training the model on the bootstrap sample
    bsm <- xgb.train(params = params,
                     data = dm,
                     nrounds = 50
                     )

    
  phat <- predict(bsm, dv, type = "prob")

  pred_rocr <- prediction(phat, y[-ind])
  auc_ROCR <- performance(pred_rocr, measure = "auc")
  auc[i] <- auc_ROCR@y.values[[1]] 
  
}

# plot auc and 95% CI of the runs
plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)
sd(auc)
mean(auc)
```


### Run the model

Set how many times our n runs is by setting a number to multiple by ncores - 1

my machine has 20 cores so it is these numbers times 19
```{r}
r1rm <- 10
v <- 100
```


### define rgrid from grid

Our grid is gonna be very small cause i dont want to run a million sets of hyperparameters
```{r, cache = TRUE, warning = FALSE}
grid <- expand.grid(
  eta = seq(0.1, 0.1, by = 0), 
  max_depth = seq(14, 18, by = 4),
  min_child_weight = seq(1, 1, by = 1), 
  subsample = seq(0.8, 0.8, by = 0), 
  colsample_bytree = seq(1, 1, by = 0),
  lambda = seq(3, 5, by = 1), 
  alpha = seq(0, 3, by = 1), 
  gamma = seq(0, 5, by = 2.5), 
  nrounds = c(50, 100, 125, 150) 
)
```

This creates a random grid
```{r, cache = TRUE, warning = FALSE}
conf_lev <- .95
num_max <- 5 # Define number around the maximum
n <- log(1-conf_lev)/log(1-num_max/nrow(grid))
ind <- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE)
rgrid <- grid[ind, ]
```

### Killing off 75% of rgrid

```{r}
kgrid <- kill_rg(xs, y, rgrid, fv = 10)
```

```{r}
k2grid <- kill_rg(xs, y, rgrid = kgrid, fv = 30)
```

```{r}
k3grid <- kill_rg(xs, y, rgrid = k2grid, fv = 50)
```

```{r}
k4grid <- kill_rg(xs, y, rgrid = k3grid, fv = 50)
```

```{r}
rgrid <- k4grid
```


### Xgboost First Fun

define our inputs
```{r, cache = TRUE, warning = FALSE}
n <- (detectCores() - 1) * r1rm
v <- v
grid <- rgrid
y <- y
xs <- xs
```

```{r, cache = TRUE, warning = FALSE}
  start_time <- Sys.time()

r1 <- run_xgb(xs, y, grid, n, v)

  # Calculate elapsed time
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
```

```{r}
head(r1)
```

## 9. Selecting the best hyperparam set

This just sets up our hyperparameters so see their frequencies etc.
```{r, cache = TRUE, warning = FALSE}
# Calculating the frequency of each hyperparameter set 
freqr <- r1 %>%
  group_by(eta, max_depth, min_child_weight, subsample, colsample_bytree, lambda, alpha, gamma, nrounds) %>%
  summarise(Frequency = n()) %>%
  ungroup()

# Calculating the average AUC_Test for each unique set of hyperparameters
avg_r1 <- r1 %>%
  group_by(eta, max_depth, min_child_weight, subsample, colsample_bytree, lambda, alpha, gamma, nrounds) %>%
  summarise(Avg_AUC_Test = mean(AUC_Test, na.rm = TRUE)) %>%
  ungroup()

# Merging the frequency data with the averaged AUC_Test scores
mr <- left_join(avg_r1, freqr, by = c("eta", "max_depth", "min_child_weight", "subsample", "colsample_bytree", "lambda", "alpha", "gamma", "nrounds"))

```

Take the best test score that has a frequency of at least th
```{r, cache = TRUE, warning = FALSE} 
th <- max(mr$Frequency)/1.25
filtered_m_r3 <- mr %>% 
  filter(Frequency >= th)
bp <- filtered_m_r3 %>% 
  filter(Avg_AUC_Test == max(filtered_m_r3$Avg_AUC_Test))
bp

```

## 10. Final Presentaion of AUC
```{r, cache = TRUE, warning = FALSE}

hp <- list(
    eta = bp[["eta"]][1],
    max_depth = bp[["max_depth"]][1],
    min_child_weight = bp[["min_child_weight"]][1],
    subsample = bp[["subsample"]][1],
    colsample_bytree = bp[["colsample_bytree"]][1],
    lambda = bp[["lambda"]][1],
    alpha = bp[["alpha"]][1],
    gamma = bp[["gamma"]][1]
)

# Set up parallel backend to use multiple cores
registerDoParallel(cores = detectCores())

r <- 1000
auc <- numeric(r)

auc <- foreach(i = 1:r, .combine = c, .packages = c("xgboost", "ROCR"), 
        .export = c("xs", "y", "hp", "bp")) %dopar% {
          
    # Bootstrap sampling
    ind <- sample(nrow(xs), nrow(xs)*1.5, replace = TRUE)
    train_xs <- xs[ind, ]
    test_xs <- xs[-ind, ]
    train_y <- y[ind]
    test_y <- y[-ind]

    # Create DMatrix for XGBoost
    dtrain <- xgb.DMatrix(data = train_xs, label = train_y)
    dtest <- xgb.DMatrix(data = test_xs, label = test_y)
    
    # Train XGBoost model
    xgbmdl <- xgb.train(params = hp, data = dtrain, nrounds = bp[["nrounds"]][1])

    # Prediction
    phat <- predict(xgbmdl, dtest)
    
    # Calculate AUC
    pred_roc <- prediction(phat, test_y)
    auc_value <- performance(pred_roc, "auc")@y.values[[1]]
    auc_value
}

# Stop the parallel backend
stopImplicitCluster()

# auc now contains the AUC scores
mean(auc)

# plot auc and 95% CI of the runs
plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)

```

Overall the results dont seem amazing compared to random forest. I would point reduced run amount for especialy running r1. This would've let us get a better final auc. also keep in mind our grid was crazy small like under 3000 rows. I think with a better grid we can definitly improve. I also dont know how i feel about the method i used to reduce grid size. Im going to look more into methodology for selecting final set of hyperparameter aswell.

```{r}
r <- 1000 # Number of bootstrap iterations
best_thresholds <- numeric(r) # To store the best threshold per iteration
j_stats <- numeric(r) # To store the max J statistic per iteration

for(i in 1:r) {
    # Bootstrap sampling
    ind <- sample(nrow(xs), size = nrow(xs), replace = TRUE)
    train_xs <- xs[ind, ]
    test_xs <- xs[-ind, ]
    train_y <- y[ind]
    test_y <- y[-ind]
    
    # Create DMatrix objects
    dtrain <- xgb.DMatrix(data = train_xs, label = train_y)
    dtest <- xgb.DMatrix(data = test_xs, label = test_y)
    
    # Train the model
    xgbmdl <- xgb.train(params = hp, data = dtrain, nrounds = bp[["nrounds"]][1])
    
    # Predictions
    phat <- predict(xgbmdl, dtest)
    
    # ROCR predictions object
    pred <- prediction(phat, test_y)
    
    # Calculate performance measures
    perf <- performance(pred, measure = "sens", x.measure = "spec")
    sensitivity <- slot(perf, "y.values")[[1]]
    specificity <- slot(perf, "x.values")[[1]]
    thresholds <- slot(perf, "alpha.values")[[1]]
    j_stat <- sensitivity + specificity - 1
    
    # Find the best threshold (maximizing J statistic)
    best_idx <- which.max(j_stat)
    best_thresholds[i] <- thresholds[best_idx]
    j_stats[i] <- j_stat[best_idx]
}

# Calculate and print the average of the best thresholds
avg_best_threshold <- mean(best_thresholds)
cat("Average Best Threshold:", avg_best_threshold, "\n")

# Plot the distribution of max J statistics for each iteration
plot(j_stats, type = "h", col = "blue", xlab = "Iteration", ylab = "Max J Statistic",
     main = "Distribution of Max J Statistics Across Bootstrap Iterations")
abline(h = mean(j_stats), col = "red", lwd = 2) # Mean J Stat line

```


JMJ