---
title: "ML_Housing_Alogorithms_SimonRaymond"
date: "2023-04-08"
---

# Simon Raymond 

Since these algorithms are only for demonstration i have run them only 10 times each.

The models i am using are LM, CART(single tree), Bagging, Random Forest, GBM(boosting).
In the near future i will add KNN, Neuralnet, AdaBoost and XGBoosting.

These codes are using Housing Data from Dr. Yigit Aydede's(SMU) RBootcamp. The data is relatively balenced and predictable. At a later time i will use unbalanced data to display under sampling, oversampling and Smoteing 

In my loops i will use Bootstrapping as much as possible instead of Cross Validation.

# Classification

I will predict housing prices and whether or not they are in the top 75th percentile

```{r, warning=FALSE}
library(RBootcamp)
data(ames)

df <- ames

# Convert Sale_Price to a binary variable
df$expensive <- ifelse(df$Sale_Price > quantile(df$Sale_Price, 0.75), 1, 0)
df$Sale_Price <- NULL
```

We see we have no NAs, our factor variables have relativly few levels.

```{r, warning=FALSE}
anyNA(df)
```

when running my GBM boosting model it produced the following warning of predictors that did not have variation. these can be removed. These collums with no variation in factor levels will actually break down a LM model.

Warning: variable 9: Utilities has no variation.
Warning: variable 70: Pool_Area has no variation.
Warning: variable 71: Pool_QC has no variation.
Warning: variable 5: Street has no variation.

```{r}
df <- df[ , -c(5, 9, 70, 71)]
```

I want to delete observations that have a factor level with less then 10 observations in that level

I have found when i run this code the deleted observations in the first run then in turn created instances of factor levels with less then 10 observations. To fix this i have looped it twice.
```{r, warning=FALSE}


for (i in 1:2){
is_fac <- sapply(df, is.factor)

for (i in names(df)[is_fac]) {
  count <- table(df[[i]])
  low_count <- names(count[count < 10])
  df <- df[!(df[[i]] %in% low_count), ]
}

}

str(df)
```

Linear Classification (Notice how good LM is, this points to the relationship being very linear)
```{r, warning=FALSE}
library(ROCR)
df_lm <- df
n <- 10

LM_auc <- c()

for (i in 1:n){
  
  idx <- sample(nrow(df_lm), nrow(df_lm), replace = TRUE)
  train <- df_lm[idx,]
  test <- df_lm[-idx, ]
  
  LM_model <- lm(expensive ~., data = train)
  
  LM_phat <- predict(LM_model, test)
  
  LM_pred_rocr <- prediction(LM_phat, test$expensive)
  LM_auc_ROCR <- performance(LM_pred_rocr, measure = "auc")
  LM_auc[i] <- LM_auc_ROCR@y.values[[1]]
                                             
}

mean(LM_auc)
sd(LM_auc)

plot(LM_auc, col="red")
abline(a = mean(LM_auc), b = 0, col = "blue", lwd = 2)
```

CART Classification
```{r, warning=FALSE}
library(rpart)
library(ROCR)
n <- 10

CART_auc <- numeric()

for (i in 1:n){
  idx <- sample(nrow(df), nrow(df), replace = TRUE)
  train <- df[idx,]
  test <- df[-idx, ]
  
  CART_model <- rpart(expensive ~., data = train)
  
  CART_phat <- predict(CART_model, test)
  
  CART_pred_rocr <- prediction(CART_phat, test$expensive)
  CART_auc_ROCR <- performance(CART_pred_rocr, measure = "auc")
  CART_auc[i] <- CART_auc_ROCR@y.values[[1]]
                                             
}

mean(CART_auc)
sd(CART_auc)

plot(CART_auc, col="red")
abline(a = mean(CART_auc), b = 0, col = "blue", lwd = 2)
```

Bagging Classification 
In reality B should be much larger
```{r, warning=FALSE}
library(randomForest)
library(ROCR)

AUC_bag <- c()

n <- 10
B <- 50

for (i in 1:n) {
ind <- sample(nrow(df), nrow(df), replace = TRUE)
train <- df[ind, ]
test <- df[-ind, ]

p = ncol(train)-1

model_bag <- randomForest(expensive ~.,
ntree = B, mtry = p, minbucket = 1, data = train) #Bagged

phat_bag <- predict(model_bag, test)

pred_rocr_bag <- prediction(phat_bag, test$expensive)
auc_ROCR_bag <- performance(pred_rocr_bag, measure = "auc")
AUC_bag[i] <- auc_ROCR_bag@y.values[[1]]
}
mean(AUC_bag)
sd(AUC_bag)

plot(AUC_bag, col="red")
abline(a = mean(AUC_bag), b = 0, col = "blue", lwd = 2)

```

Random Forest Classification 
In reality B should be much larger
```{r, warning=FALSE}
library(randomForest)
library(ROCR)

AUC_rf <- c()

n <- 10
B <- 50

for (i in 1:n) {
ind <- sample(nrow(df), nrow(df), replace = TRUE)
train <- df[ind, ]
test <- df[-ind, ]


model_rf <- randomForest(expensive ~.,
                         ntree = B, 
                         data = train) 

phat_rf <- predict(model_rf, test)

pred_rocr_rf <- prediction(phat_rf, test$expensive)
auc_ROCR_rf <- performance(pred_rocr_rf, measure = "auc")
AUC_rf[i] <- auc_ROCR_rf@y.values[[1]]
}
mean(AUC_rf)
sd(AUC_rf)

plot(AUC_rf, col="red")
abline(a = mean(AUC_rf), b = 0, col = "blue", lwd = 2)

```

GBM Boosting Classification
I ran this multiple times with different ranges and landed close to these parameters. It is important to note with this algoithm we can either broaden the search range or work in random selection for the hyperparameters outside of the loop
```{r}
library(gbm)

h <- seq(0.09, 0.10, 0.01)
B <- c(700, 750)
D <- c(1:2)

runs <- 10

grid <- as.matrix(expand.grid(D, B, h))

n <- nrow(grid)


Opt_param <- c()
final_AUC_list_boost <- c()

for (j in 1:runs){
  ind_boost_test <- sample(nrow(df), nrow(df)*0.9)
  model_data_boost <- df[ind_boost_test, ]
  test_boost <- df[-ind_boost_test, ]
  
  AUC_boost_tuning <- c()
  for (i in 1:n){
    ind_boost <- sample(nrow(model_data_boost), nrow(model_data_boost), replace = TRUE)
    train_boost <- model_data_boost[ind_boost, ]
    val_boost <- model_data_boost[-ind_boost, ]
    
    boost_model <- gbm(expensive~., 
                       distribution ="bernoulli", 
                       n.trees= grid[i, 2],
                       interaction.depth = grid[i, 1], 
                       shrinkage = grid[i, 3],
                       bag.fraction = 1,
                       data = train_boost)
    
    phat_boost <- predict(boost_model, n.trees = grid[i, 2], val_boost, type = "response")
    
    pred_rocr_boost <- prediction(phat_boost, val_boost$expensive)
    auc_ROCR_boost <- performance(pred_rocr_boost, measure = "auc")
    AUC_boost_tuning[i] <- auc_ROCR_boost@y.values[[1]]
  }
  
  best_index <- which.max(AUC_boost_tuning)
  best_AUC_boost <- AUC_boost_tuning[best_index]
  best_params_boost <- grid[best_index, ]
  
Opt_param[[j]] <- best_params_boost
  
  model_test <- gbm(expensive~., 
                       distribution ="bernoulli", 
                       n.trees= best_params_boost[2],
                       interaction.depth = best_params_boost[1], 
                       shrinkage = best_params_boost[3], 
                       bag.fraction = 1,
                       data = model_data_boost)
  
  
  phat_final_boost <- predict(model_test, test_boost, n.trees = best_params_boost[2], type = "response")
  
  pred_rocr_final_boost <- prediction(phat_final_boost, test_boost$expensive)
  AUC_ROCR_boost_test <- performance(pred_rocr_final_boost, measure = "auc")
  final_AUC_list_boost[j] <- AUC_ROCR_boost_test@y.values[[1]]
}

final_AUC_boost <- mean(final_AUC_list_boost)
final_sd_boost <- sd(final_AUC_list_boost)
final_params_boost <- do.call(rbind, Opt_param)


final_params_boost
final_sd_boost
final_AUC_boost

plot(final_AUC_list_boost, col="red")
abline(a = mean(final_AUC_list_boost), b = 0, col = "blue", lwd = 2)
```

# Regresssion 

I will work on the same data but instead of classification i will work regressionally.
```{r}
rm(list = ls())


library(RBootcamp)
data(ames)

df <- ames
```

```{r, warning=FALSE}
df <- df[ , -c(5, 9, 70, 71)]
```

```{r, warning=FALSE}
for (i in 1:2){
factor_vars <- sapply(df, is.factor)

for (i in names(df)[factor_vars]) {
  counts_table <- table(df[[i]])
  low_count <- names(counts_table[counts_table < 10])
  df <- df[!(df[[i]] %in% low_count), ]
}
}
```

Linear Regresional
```{r, warning=FALSE}

n <- 10

RMSPE_LM <- c()

for (i in 1:n){
  
  idx <- sample(nrow(df), nrow(df), replace = TRUE)
  train <- df[idx,]
  test <- df[-idx, ]
  
  LM_model <- lm(Sale_Price~., data = train)
  
  LM_yhat <- predict(LM_model, test)
  
RMSPE_LM[i] <- sqrt(mean((test$Sale_Price - LM_yhat)^2))
                                             
}

mean(RMSPE_LM)
sd(RMSPE_LM)

plot(RMSPE_LM, col="red")
abline(a = mean(RMSPE_LM), b = 0, col = "blue", lwd = 2)
```

CART Regresional
```{r, warning=FALSE}
library(rpart)

n <- 10

RMSPE_CART <- c()

for (i in 1:n){
  idx <- sample(nrow(df), nrow(df), replace = TRUE)
  train <- df[idx,]
  test <- df[-idx, ]
  
  CART_model <- rpart(Sale_Price ~., data = train)
  
  CART_yhat <- predict(CART_model, test)
  
RMSPE_CART[i] <- sqrt(mean((test$Sale_Price - CART_yhat)^2))

                                             
}

mean(RMSPE_CART)
sd(RMSPE_CART)

plot(RMSPE_CART, col="red")
abline(a = mean(RMSPE_CART), b = 0, col = "blue", lwd = 2)
```

Bagging Regressional 
```{r, warning=FALSE}
library(randomForest)

RMSPE_bag <- c()

n <- 10
B <- 50

for (i in 1:n) {
ind <- sample(nrow(df), nrow(df), replace = TRUE)
train <- df[ind, ]
test <- df[-ind, ]

p = ncol(train)-1

model_bag <- randomForest(Sale_Price ~.,
ntree = B, mtry = p, minbucket = 1, data = train) #Bagged

yhat_bag <- predict(model_bag, test)

RMSPE_bag[i] <- sqrt(mean((test$Sale_Price - yhat_bag)^2))


}
mean(RMSPE_bag)
sd(RMSPE_bag)

plot(RMSPE_bag, col="red")
abline(a = mean(RMSPE_bag), b = 0, col = "blue", lwd = 2)
```

Random Forest Regressional  
```{r, warning=FALSE}
library(randomForest)

RMSPE_rf <- c()

n <- 10
B <- 50

for (i in 1:n) {
ind <- sample(nrow(df), nrow(df), replace = TRUE)
train <- df[ind, ]
test <- df[-ind, ]


model_rf <- randomForest(Sale_Price ~.,
ntree = B, data = train) #rf

yhat_rf <- predict(model_rf, test)

RMSPE_rf[i] <- sqrt(mean((test$Sale_Price - yhat_rf)^2))


}
mean(RMSPE_rf)
sd(RMSPE_rf)

plot(RMSPE_rf, col="red")
abline(a = mean(RMSPE_rf), b = 0, col = "blue", lwd = 2) 
```

BOOSTING Regressional 
I ran this multiple times and these are the general parameter ranges a narrowed in on.
```{r, warning=FALSE}
library(gbm)

h <- seq(0.11, 0.13, 0.01)
B <- c(500, 550)
D <- c(1:2)

runs <- 10

grid <- as.matrix(expand.grid(D, B, h))

n <- nrow(grid)

Opt_param <- c()
RMSPE_test <- c()

for (j in 1:runs){
  ind_boost_test <- sample(nrow(df), nrow(df)*0.9)
  model_data_boost <- df[ind_boost_test, ]
  test_boost <- df[-ind_boost_test, ]
  
  RMSPE_boost_tuning <- c()
  for (i in 1:n){
    ind_boost <- sample(nrow(model_data_boost), nrow(model_data_boost), replace = TRUE)
    train_boost <- model_data_boost[ind_boost, ]
    val_boost <- model_data_boost[-ind_boost, ]
    
    boost_model <- gbm(Sale_Price~., 
                       distribution ="gaussian", 
                       n.trees= grid[i, 2],
                       interaction.depth = grid[i, 1], 
                       shrinkage = grid[i,3],
                       bag.fraction = 1,
                       data = train_boost)
    
    yhat_boost <- predict(boost_model, n.trees = grid[i, 2], val_boost, type = "response")

    RMSPE_boost_tuning[i] <- sqrt(mean((val_boost$Sale_Price - yhat_boost)^2))
  }
  best_RMSPE_boost <- RMSPE_boost_tuning[which.min(RMSPE_boost_tuning)]
  best_params_boost <- grid[which.min(RMSPE_boost_tuning), ]
  
  
  Opt_param[[j]] <- best_params_boost
  
  model_test <- gbm(Sale_Price ~., 
                       distribution ="gaussian", 
                       n.trees= best_params_boost[2],
                       interaction.depth = best_params_boost[1], 
                       shrinkage = best_params_boost[3], 
                       bag.fraction = 1,
                       data = model_data_boost)
  
  yhat_boost_final <- predict(model_test, n.trees = best_params_boost[2], test_boost, type = "response")
  
  RMSPE_test[j] <- sqrt(mean((test_boost$Sale_Price - yhat_boost_final)^2))

}

final_RMSPE_boost <- mean(RMSPE_test)
final_sd_boost <- sd(RMSPE_test)
final_params_boost <- do.call(rbind, Opt_param)

final_params_boost
final_sd_boost
final_RMSPE_boost

plot(RMSPE_test, col="red")
abline(a = mean(RMSPE_test), b = 0, col = "blue", lwd = 2)
```
