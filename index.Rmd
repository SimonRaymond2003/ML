---
title: "Machine Learning Codes and Process"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: readable
    highlight: tango
date: "2024-01-28"
navbar:
  title: "Simon Raymond"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
    - text: "Titanic XGBoosting"
      href: Titanic_XGBoosting.html
    - text: "Titanic Adaboost"
      href: Titanic_Adaboost.html
    - text: "Titanic Nnet"
      href: Titanic_Nnet.html
    - text: "Titanic_LM_CART_and_RF"
      href: Titanic_LM_CART_RF.html
    - text: "XGB_2"
      href: XGB_2.html
    - text: "Speeds"
      href: Speeds.html
name: SimonRaymond
---

### Simon P. Raymond

For all my codes, I will try to follow similar patterns.

I use bootstrapping in all of my codes instead of k-fold-cv. While it creates more work as many functions have k-fold-cv built-in, I feel it is worth it as it is a more robust way of getting validation samples and allows me to make as many as I want.

For the initial train/val/test process, the train/val will be bootstrapped a number of times to ensure I'm getting a relatively representative display of what each hyperparameter actually is. Then, I will test the best average hyperparameter (based on some metric) on a set-aside test set. This entire process will repeat N times. 

XGB2 follows a more accurate realistic way of tuning which tests each hyperparmeter a large amount of times.

### Terminology that I use

* Train: This is the data that I "build" the model on. It is the data that the model "sees."

* Val: This is validation data that the model predicts upon that gives us results in terms of tuning hyperparameters.

* Test: This is the set-aside data that we evaluate our model upon.

* md/model_data: This is the combination of train and val data that we build our model on during the final "test" stage of the algorithm.

* AUC: Area under Curve.

* y: I will always make the predicted value y.

Titanic: For these examples, I am using the classic ML Titanic data to predict survivors.

### Content

Adaboost

it uses properly worked with titanic features, No parrallel processing it is just to give a super broad look.

Xgboost

I have two files.. One does more analysis and shows the data prep a bit better. The other appliies a more efficient algorithm.
it uses properly worked with titanic features

Nnet

it uses properly worked with titanic features, No parrallel processing it is just to give a super broad look.


LM RF and Cart

These features are not properly worked with as they are not as important. A better RF model is found in the importance of features section in xgboost. They do use Parrallel proccessing

### notes to myself

Make a seperate page for all my data. it'll be cleaner

Finish speeds.

put jstat on xgb2.

Fix the typos in xgboost at the end

shun all start up messages etc