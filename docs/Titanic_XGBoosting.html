<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Titanic XGBoosting</title>

<script src="site_libs/header-attrs-2.19/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Simon Raymond</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="Titanic_XGBoosting.html">Titanic_XGBoosting</a>
</li>
<li>
  <a href="Titanic_Adaboost.html">Titanic_Adaboost</a>
</li>
<li>
  <a href="Titanic_Nnet.html">Titanic_Nnet</a>
</li>
<li>
  <a href="Titanic_LM_CART_RF.html">Titanic_LM_CART_and_RF</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Titanic XGBoosting</h1>

</div>


<p>This will be the file for my titanic xgb</p>
<div id="data-prep" class="section level1">
<h1>Data Prep</h1>
<p>Download the Data</p>
<pre class="r"><code>library(readr)

train_titanic &lt;- read_csv(&quot;train.csv&quot;)</code></pre>
<pre><code>## Rows: 891 Columns: 12
## ── Column specification ─────────────────────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## chr (5): Name, Sex, Ticket, Cabin, Embarked
## dbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<p>The collums i will use are y, Pclass, Sec, Age, SibSp, Parch, Fare,
Emarked as they are the easiest to work with. other possibilities
include using the titles for names</p>
<pre class="r"><code>library(dplyr)

data &lt;- train_titanic[ ,c(2, 3, 5, 6, 7, 8, 10, 12)]
data &lt;- data %&gt;% rename(y = Survived)</code></pre>
<p>convert the characters to factors. convert our y variable to numeric
for XGBoosting</p>
<pre class="r"><code>char_cols &lt;- sapply(data, is.character)
data[, char_cols] &lt;- lapply(data[, char_cols], factor)
data$y &lt;- as.numeric(data$y)</code></pre>
<p>we have NAs we need to deal with</p>
<pre class="r"><code>colSums(is.na(data))</code></pre>
<pre><code>##        y   Pclass      Sex      Age    SibSp    Parch     Fare Embarked 
##        0        0        0      177        0        0        0        2</code></pre>
<p>First i am going to predict the NAs for Age with a simple Random
Forest model.</p>
<pre class="r"><code>na_index &lt;- which(is.na(data$Age))#index the missing NAs
na_data &lt;- data[na_index, ]#create a data frame of the NAs indexed
c_data &lt;-data[-na_index, ]#take the complete data to c_data
na_data &lt;- na_data[, -which(names(data) == &quot;Age&quot;)] # take out the collum that only has NAs in it</code></pre>
<p>i will use na.roughfix() for the two missing Embarked values</p>
<pre class="r"><code>library(randomForest)

c_data &lt;- na.roughfix(c_data)# there are two NAs in embarked
na_data &lt;-  na.roughfix(na_data)</code></pre>
<p>Predict our NAs</p>
<pre class="r"><code>library(randomForest)

model_rf &lt;- randomForest(Age ~.,
                         ntree = 1200, 
                         data = c_data) 

pred_na &lt;- predict(model_rf, na_data)</code></pre>
<p>This binds our predictions to the corresponding index number for each
missing NA in “data”</p>
<pre class="r"><code>na_n_ind &lt;- cbind(pred_na, na_index)

data$Age[na_n_ind[, 2]] &lt;- na_n_ind[, 1]</code></pre>
<p>finnaly just use na.roughfix() for the two embarked values in the
actualy data sets</p>
<pre class="r"><code>library(randomForest)

data &lt;- na.roughfix(data)
colSums(is.na(data))</code></pre>
<pre><code>##        y   Pclass      Sex      Age    SibSp    Parch     Fare Embarked 
##        0        0        0        0        0        0        0        0</code></pre>
<pre class="r"><code>str(data)</code></pre>
<pre><code>## tibble [891 × 8] (S3: tbl_df/tbl/data.frame)
##  $ y       : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass  : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...
##  $ Sex     : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age     : num [1:891] 22 38 26 35 35 ...
##  $ SibSp   : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch   : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...
##  $ Fare    : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...
##  $ Embarked: Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 1 3 3 3 2 3 3 3 1 ...</code></pre>
</div>
<div id="xgboosting" class="section level1">
<h1>xgBoosting</h1>
<p>This code performs hyperparameter tuning for an XGBoost model using a
bootstrapping approach. The number of iterations is controlled by the
variable n. At each iteration, the code samples s hyperparameter
combinations from a pre-defined grid. Then, the data is split into
training and validation sets, and the selected hyperparameters are
evaluated v times on the validation set.</p>
<p>For each hyperparameter combination, the average area under the curve
(AUC) is calculated across the v validation runs. The hyperparameter
combination with the highest average AUC is selected as the best
hyperparameters for that iteration. The selected hyperparameters and
their corresponding AUC are stored in the opt matrix.</p>
<p>Finally, the code trains an XGBoost model on the entire model data
set using the best hyperparameters from each iteration. The AUC of the
trained model is evaluated on a hold-out test set, and the results are
stored in opt. A weakness i have found is that the final test on be
validated hyperparameters is only preformed once since the data is kept
seperate.</p>
<p>Note that the hyperparameters being tuned are nrounds, max_depth,
eta, subsample, colsample_bytree, gamma, min_child_weight, alpha, and
lambda. The nthread parameter controls the number of threads to use
during training.</p>
<p>Overall, this code can be useful for finding the best set of
hyperparameters for an XGBoost model. The bootstrapping approach can
help mitigate overfitting, and the use of a pre-defined grid can help
avoid an exhaustive search of the hyperparameter space.</p>
<p>This grid is very limited because of processing power. it is
relativly tuned.</p>
<pre class="r"><code>grid &lt;- expand.grid(
  nrounds = seq(300, 300, by = 100),
  max_depth = seq(13, 16, by = 2),
  eta = seq(0.1, 0.2, by = 0.01),
  subsample = seq(0.7, 0.8, by = 0.1),
  colsampe_bytree = seq(0.7, 0.8, by = 0.1),
  gamma = seq(1, 2, by = 1),
  min_child_weight = seq(0, 2, by = 1),
  alpha = seq(0, 1, by = 1),
  lambda = seq(0, 1, by = 1)
)</code></pre>
<pre class="r"><code>library(xgboost)</code></pre>
<pre><code>## 
## Attaching package: &#39;xgboost&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice</code></pre>
<pre class="r"><code>library(ROCR)

start_time &lt;- Sys.time()

n &lt;- 20
s &lt;- 250
v &lt;- 5
esr &lt;- 10
opt &lt;- matrix(0, nrow = n, ncol = 11, dimnames = list(NULL, c(&quot;nrounds&quot;, &quot;max_depth&quot;, &quot;eta&quot;, &quot;subsample&quot;, &quot;colsampe_bytree&quot;, &quot;gamma&quot;, &quot;min_child_weight&quot;, &quot;alpha&quot;, &quot;lambda&quot;, &quot;val_AUC&quot;, &quot;Test_AUC&quot;)))

auc_t &lt;- c()
for(j in 1:n){
  
    

  
  ind &lt;- sample(nrow(grid), s, replace = FALSE)
  rgrid &lt;- grid[ind, ]
  

  ind1 &lt;- sample(nrow(data), nrow(data)*0.8)
  md &lt;- data[ind1, ]
  test &lt;- data[-ind1, ]
  
  auc_runs &lt;- c()
  for (i in 1:nrow(rgrid)) {
    auc_tuning &lt;- c()
for (p in 1:v){
  
  #cat(&quot;loops: &quot;, j, i, p, &quot;\r&quot;)
  
  # Initial Split
  ind2 &lt;- unique(sample(nrow(md), nrow(md), replace = TRUE))
  train &lt;- md[ind2, ]
  val &lt;- md[-ind2, ]

  # One-hot coding using R&#39;s `model.matrix`
  train_x &lt;- model.matrix(~.+0, data = train[, -which(names(train) == &quot;y&quot;)]) 
  train_y &lt;- train$y
 
  val_x &lt;- model.matrix(~.+0, data = val[, -which(names(val) == &quot;y&quot;)])
  val_y &lt;- val$y
  
  # Preparing efficient matrix
  xgb_train &lt;- xgb.DMatrix(data = train_x, label = train_y) 
  xgb_val &lt;- xgb.DMatrix(data = val_x, label = val_y)

  # Training and evaluating the model
  xgb_model &lt;- xgb.train(params = list(objective = &quot;binary:logistic&quot;),
                         data = xgb_train, 
                         nrounds = rgrid[i, 1],
                         max_depth = rgrid[i, 2],
                         eta = rgrid[i, 3], 
                         subsample = rgrid[i, 4],
                         colsample_bytree = rgrid[i, 5],
                         gamma = rgrid[i, 6],
                         min_child_weight = rgrid[i, 7],
                         alpha = rgrid[i, 8],
                         lambda = rgrid[i, 9],
                         early_stopping_rounds = esr,
                         eval_metric = &quot;auc&quot;,
                         watchlist = list(train = xgb_train, val = xgb_val),
                         verbose = FALSE,
                         nthread = 3
  )

  
# Predicting on the val set
phat &lt;- predict(xgb_model, xgb_val, type = &quot;prob&quot;)

  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, val_y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc_tuning[p] &lt;- auc_ROCR@y.values[[1]]
}
auc_runs[i] &lt;- mean(auc_tuning)



 BI &lt;- which.max(auc_runs) #best hyperparameter index
  best_AUC &lt;- auc_runs[BI]
  best_params &lt;- rgrid[BI, ]
  
  opt[j, 1] &lt;- best_params[1, 1]
  opt[j, 2] &lt;- best_params[1, 2]
  opt[j, 3] &lt;- best_params[1, 3]
  opt[j, 4] &lt;- best_params[1, 4]
  opt[j, 5] &lt;- best_params[1, 5]
  opt[j, 6] &lt;- best_params[1, 6]
  opt[j, 7] &lt;- best_params[1, 7]
  opt[j, 8] &lt;- best_params[1, 8]
  opt[j, 9] &lt;- best_params[1, 9]
  opt[j, 10] &lt;- max(auc_runs)
}

  # One-hot coding using R&#39;s `model.matrix`
  md_x &lt;- model.matrix(~.+0, data = md[, -which(names(md) == &quot;y&quot;)]) 
  md_y &lt;- md$y
 
  test_x &lt;- model.matrix(~.+0, data = test[, -which(names(test) == &quot;y&quot;)])
  test_y &lt;- test$y
  
  # Preparing efficient matrix
  xgb_md &lt;- xgb.DMatrix(data = md_x, label = md_y) 
  xgb_test &lt;- xgb.DMatrix(data = test_x, label = test_y)
  
  
  
  
  # Training and evaluating the model
  xgb_mt &lt;- xgb.train(params = list(objective = &quot;binary:logistic&quot;),
                         data = xgb_md, 
                         nrounds = opt[j ,1],
                         max_depth = opt[j ,2],
                         eta = opt[j ,3], 
                         subsample = opt[j ,4],
                         colsample_bytree = opt[j ,5],
                         gamma = opt[j ,6],
                         min_child_weight = opt[j ,7],
                         alpha = opt[j ,8],
                         lambda = opt[j ,9],
                         early_stopping_rounds = esr,
                         eval_metric = &quot;auc&quot;,
                         watchlist = list(train = xgb_md, val = xgb_test),
                         verbose = FALSE,
                         nthread = 3
  )

  
# Predicting on the test set
phat &lt;- predict(xgb_mt, xgb_test, type = &quot;prob&quot;)

  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, test_y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc_t &lt;- auc_ROCR@y.values[[1]]



opt[j, 11] &lt;- auc_t
  
}



end_time &lt;- Sys.time()

elapsed_time &lt;- end_time - start_time

elapsed_time</code></pre>
<pre><code>## Time difference of 51.67212 mins</code></pre>
<p>This code will index and assign the best hyper parameters from opt
into a matrix called win.</p>
<p>NOTE: A weakness is that the test run is one split or rather one n
run. this means it is weak to a high standard deviation or variability
in the models predicition. The issue is that if we want to be able to
use multiple bootstrapped samples to get a average test run we will
either risk overfitting the hyperparameters or will have to split the
data in a awkward way.</p>
<p>Im want to do more reasearch on if there is a way boostrap samples
for a average and not overfit the hyperparameters/model</p>
<p>This takes the top 50 percent of hyperparameters based on there</p>
<pre class="r"><code>th &lt;- quantile(opt[,11], probs = 0.50)
ind_opt &lt;- which(opt[,11] &gt;= th)

win &lt;- opt[ind_opt, ]
win</code></pre>
<pre><code>##       nrounds max_depth  eta subsample colsampe_bytree gamma min_child_weight
##  [1,]     300        15 0.17       0.8             0.8     1                2
##  [2,]     300        13 0.19       0.8             0.8     2                0
##  [3,]     300        13 0.15       0.8             0.7     2                1
##  [4,]     300        13 0.12       0.7             0.7     1                2
##  [5,]     300        13 0.18       0.7             0.8     1                0
##  [6,]     300        13 0.12       0.8             0.7     2                0
##  [7,]     300        15 0.16       0.7             0.7     1                1
##  [8,]     300        15 0.17       0.7             0.7     1                0
##  [9,]     300        13 0.19       0.8             0.8     2                1
## [10,]     300        15 0.19       0.8             0.8     2                0
##       alpha lambda   val_AUC  Test_AUC
##  [1,]     0      0 0.8908953 0.9134716
##  [2,]     1      0 0.8990799 0.9007409
##  [3,]     0      1 0.8925681 0.9216033
##  [4,]     1      0 0.8904513 0.9192308
##  [5,]     0      1 0.9023397 0.9010528
##  [6,]     0      1 0.8967614 0.9096482
##  [7,]     0      1 0.8928044 0.8928193
##  [8,]     1      0 0.8969338 0.9085598
##  [9,]     0      1 0.8957260 0.9231017
## [10,]     0      1 0.9080930 0.8918919</code></pre>
<p>This code is using bootstrapping and runs the best hyperparamters
that we picked before. it tests the selected hyper parameters n times.
since we are not tuning in it we can use all of the data in it.</p>
<pre class="r"><code>library(xgboost)
library(ROCR)

n &lt;- 400
w &lt;- nrow(win)
esr &lt;- 10
#collums are hyper param set
bs_results &lt;- matrix(0, nrow = n, ncol = w)



for (i in 1:n) {
  for (j in 1:w){
    auc &lt;- c()
  #cat(&quot;loops: &quot;, i, j, &quot;\r&quot;)
  
  # Initial Split
  ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE))
  mdf&lt;- data[ind, ]
  test &lt;- data[-ind, ]
  
  # One-hot coding using R&#39;s `model.matrix`
  md_y &lt;- mdf$y
  test_y &lt;- test$y
  md_x &lt;- model.matrix(~.+0, data = mdf[, -which(names(mdf) == &quot;y&quot;)]) 
  test_x &lt;- model.matrix(~.+0, data = test[, -which(names(test) == &quot;y&quot;)])
  
  # Preparing efficient matrix
  xgb_md &lt;- xgb.DMatrix(data = md_x, label = md_y) 
  xgb_test &lt;- xgb.DMatrix(data = test_x, label = test_y)
  
  # Training and evaluating the model
  xgb_m &lt;- xgb.train(params = list(objective = &quot;binary:logistic&quot;),
                               data = xgb_md, 
                               nrounds = win[j, 1],
                               max_depth = win[j, 2],
                               eta = win[j, 3], 
                               subsample = win[j, 4],
                               colsample_bytree = win[j, 5],
                               gamma = win[j, 6],
                               min_child_weight = win[j, 7],
                               alpha = win[j, 8],
                               lambda = win[j, 9],
                               early_stopping_rounds = esr,
                               eval_metric = &quot;auc&quot;,
                               watchlist = list(train = xgb_md, val = xgb_test),
                               verbose = FALSE,
                               nthread = 3
                              
  )
  
  # Predicting on the test set
  phat &lt;- predict(xgb_m, xgb_test, type = &quot;prob&quot;)
  
  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, test_y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc &lt;- auc_ROCR@y.values[[1]]
  
 bs_results[i, j] &lt;- auc 
}
}</code></pre>
<p>This code takes the colmeans of the matrix that gave our results.
then we bind these results onto the win matrix for presentation. it also
takes the index of the best run from before and saves the corelating
hyperparameters into ww.</p>
<pre class="r"><code>bh_ind &lt;- which.max(colMeans(bs_results))
ww &lt;- win[bh_ind, ]
mean(bs_results[,bh_ind])</code></pre>
<pre><code>## [1] 0.8865437</code></pre>
<pre class="r"><code>cm &lt;- as.vector(colMeans(bs_results))
win &lt;- cbind(win, av_ovr_n_runs=cm)</code></pre>
<p>This is the final run. it uses bootrapping and runs n times.</p>
<pre class="r"><code>library(xgboost)
library(ROCR)

n &lt;- 1000


esr &lt;- 10


 auc &lt;- c()
for (i in 1:n) {

   
  #cat(&quot;loops: &quot;, i, &quot;\r&quot;)
  
  # Initial Split This is bootstrapping
  ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE))
  mdf&lt;- data[ind, ]
  test &lt;- data[-ind, ]
  
  # One-hot coding using R&#39;s `model.matrix`
  md_y &lt;- mdf$y
  test_y &lt;- test$y
  md_x &lt;- model.matrix(~.+0, data = mdf[, -which(names(mdf) == &quot;y&quot;)]) 
  test_x &lt;- model.matrix(~.+0, data = test[, -which(names(test) == &quot;y&quot;)])
  
  # Preparing efficient matrix
  xgb_md &lt;- xgb.DMatrix(data = md_x, label = md_y) 
  xgb_test &lt;- xgb.DMatrix(data = test_x, label = test_y)
  
  # Modeling
  xgb_m &lt;- xgb.train(params = list(objective = &quot;binary:logistic&quot;),
                               data = xgb_md, 
                               nrounds = ww[1],
                               max_depth = ww[2],
                               eta = ww[3], 
                               subsample = ww[4],
                               colsample_bytree = ww[5],
                               gamma = ww[6],
                               min_child_weight = ww[7],
                               alpha = ww[8],
                               lambda = ww[9],
                               early_stopping_rounds = esr,
                               eval_metric = &quot;auc&quot;,
                               watchlist = list(train = xgb_md, val = xgb_test),
                               verbose = FALSE,
                               nthread = 3
                              
  )
  
  # Predicting on the test set
  phat &lt;- predict(xgb_m, xgb_test, type = &quot;prob&quot;)
  
  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, test_y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc[i] &lt;- auc_ROCR@y.values[[1]]
}
 
 mean(auc)</code></pre>
<pre><code>## [1] 0.8856845</code></pre>
<pre class="r"><code> sd(auc)</code></pre>
<pre><code>## [1] 0.01739754</code></pre>
<p>This presents the mean and 95% CI of the model runs Note: we are
representing individual observations/model runs so we use SD not SE.</p>
<pre class="r"><code># plot auc and mean
plot(auc, col=&quot;red&quot;)
abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3)</code></pre>
<p><img src="Titanic_XGBoosting_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>If we are finding a threshold he then need to pay attention to the
watchlist outputs and tune nrounds accordingly</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
