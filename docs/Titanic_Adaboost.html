<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Titanic Adaboost</title>

<script src="site_libs/header-attrs-2.19/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Simon Raymond</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="Titanic_XGBoosting.html">Titanic_XGBoosting</a>
</li>
<li>
  <a href="Titanic_Adaboost.html">Titanic_Adaboost</a>
</li>
<li>
  <a href="Titanic_Nnet.html">Titanic_Nnet</a>
</li>
<li>
  <a href="Titanic_LM_CART_RF.html">Titanic_LM_CART_and_RF</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Titanic Adaboost</h1>

</div>


<p>This wil be my titanic Adaboost page</p>
<p>Download the Data</p>
<pre class="r"><code>library(readr)

train_titanic &lt;- read_csv(&quot;train.csv&quot;)</code></pre>
<pre><code>## Rows: 891 Columns: 12
## ── Column specification ───────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## chr (5): Name, Sex, Ticket, Cabin, Embarked
## dbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<p>the columns i will use are y(Survived), Pclass, Sex, Age, SibSp,
Parch, Fare, Emarked as they are the easiest to work with. other
possibilities include using the titles for names</p>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>selected_cols &lt;- c(&quot;Survived&quot;, &quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;SibSp&quot;, &quot;Parch&quot;, &quot;Fare&quot;, &quot;Embarked&quot;)
data &lt;- train_titanic[selected_cols]
data &lt;- data %&gt;% rename(y = Survived)</code></pre>
<p>convert the characters to factors.</p>
<pre class="r"><code>char_cols &lt;- sapply(data, is.character)
data[, char_cols] &lt;- lapply(data[, char_cols], factor)
data$y &lt;- as.numeric(data$y)</code></pre>
<p>we have NAs we need to deal with</p>
<pre class="r"><code>colSums(is.na(data))</code></pre>
<pre><code>##        y   Pclass      Sex      Age    SibSp    Parch     Fare Embarked 
##        0        0        0      177        0        0        0        2</code></pre>
<p>First i am going to predict the NAs for Age with a simple Random
Forest model.</p>
<pre class="r"><code>na_index &lt;- which(is.na(data$Age))#index the missing NAs
na_data &lt;- data[na_index, ]#create a data frame of the NAs indexed
c_data &lt;-data[-na_index, ]#take the complete data to c_data
na_data &lt;- na_data[, -which(names(data) == &quot;Age&quot;)] # take out the collum that only has NAs in it</code></pre>
<p>i will use na.roughfix() for the two missing Embarked values</p>
<pre class="r"><code>library(randomForest)</code></pre>
<pre><code>## randomForest 4.7-1.1</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>c_data &lt;- na.roughfix(c_data)# there are two NAs in embarked
na_data &lt;-  na.roughfix(na_data)</code></pre>
<p>Predict our NAs</p>
<pre class="r"><code>library(randomForest)

model_rf &lt;- randomForest(Age ~.,
                         ntree = 1200, 
                         data = c_data) 

pred_na &lt;- predict(model_rf, na_data)</code></pre>
<p>This binds our predictions to the corresponding index number for each
missing NA in “data”</p>
<pre class="r"><code>na_n_ind &lt;- cbind(pred_na, na_index)

data$Age[na_n_ind[, 2]] &lt;- na_n_ind[, 1]</code></pre>
<p>finally just use na.roughfix() for the two embarked values since it
is so little</p>
<pre class="r"><code>library(randomForest)

data &lt;- na.roughfix(data)
colSums(is.na(data))</code></pre>
<pre><code>##        y   Pclass      Sex      Age    SibSp    Parch     Fare Embarked 
##        0        0        0        0        0        0        0        0</code></pre>
<p>y must be in first collomn</p>
<pre class="r"><code>str(data)</code></pre>
<pre><code>## tibble [891 × 8] (S3: tbl_df/tbl/data.frame)
##  $ y       : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass  : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...
##  $ Sex     : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age     : num [1:891] 22 38 26 35 35 ...
##  $ SibSp   : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch   : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...
##  $ Fare    : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...
##  $ Embarked: Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 1 3 3 3 2 3 3 3 1 ...</code></pre>
<p>ADABOOST in gbm</p>
<ul>
<li>Another option and thing i want to compare is JOUSboost</li>
</ul>
<p>This creates our Grid</p>
<pre class="r"><code>interaction.depth &lt;- 2:6


grid &lt;- as.matrix(expand.grid(
  shrinkage &lt;- seq(0.005, 0.25, 0.005), 
  n.trees &lt;- seq(100, 2000, 100), 
  interaction.depth &lt;- c(2:6)

  )
  )</code></pre>
<p>This code performs hyperparameter tuning for an Adaboost model in the
GBM function/package. The number of iterations is controlled by the
variable n. At each iteration, the code samples s hyperparameter
combinations from a pre-defined grid. Then, the data is split into
modeling and test data sets. the modeling data is boostrapped to make
train and val data sets to to be used to select hyperparameters to be
tested.</p>
<p>For each hyperparameter combination, the average area under the curve
(AUC) is calculated across the v validation runs. The hyperparameter
combination with the highest average AUC is selected as the best
hyperparameters for that iteration. The selected hyperparameters and
their corresponding AUC are stored in the opt matrix.</p>
<p>Finally, the code trains an Adaboostt model on the modeldata(both
train and val) set using the best hyperparameters from each iteration.
The AUC of that model is evaluated on a hold-out test set, and the
results are stored in opt.</p>
<p>Note that the hyperparameters being tuned are shrinkage, n.trees, and
interaction.depth. The nthread parameter controls the number of threads
to use during training.</p>
<pre class="r"><code>library(gbm)</code></pre>
<pre><code>## Loaded gbm 2.1.8.1</code></pre>
<pre class="r"><code>library(ROCR)


n &lt;- 50 # how many times we repeat the algorithm
s &lt;- 500 # how many hyperparamter sets are validated, length of randomgrid
v &lt;- 10 # how many times we bootstrapped a sample for each hyperparameter set



opt &lt;- matrix(0, nrow = n, ncol = 5, dimnames = list(NULL, c(&quot;shrinkage&quot;, &quot; n.trees&quot;, &quot;interaction.depth&quot;, &quot;val_AUC&quot;, &quot;Test_AUC&quot;)))

auc_t &lt;- c()

for(j in 1:n){

# create the random grind
  ind &lt;- sample(nrow(grid), s, replace = FALSE)
  rgrid &lt;- grid[ind, ]
  
# create model data and test data
  ind1 &lt;- sample(nrow(data), nrow(data)*0.8)
  md &lt;- data[ind1, ]
  test &lt;- data[-ind1, ]
  
    V_auc_mean &lt;- c()
  for (i in 1:nrow(rgrid)){
    #cat(&quot;loops: &quot;, j, i, &quot;\r&quot;)
    
    auc_tuning &lt;- c()
    for (p in 1:v){
        # bootstrap samples for train and val
  ind2 &lt;- unique(sample(nrow(md), nrow(md), replace = TRUE))
  train &lt;- md[ind2, ]
  val &lt;- md[-ind2, ]
  
  # create a model off of train data
      model &lt;- gbm(y ~ ., 
                   distribution = &quot;adaboost&quot;, 
                   bag.fraction = 1, 
                   shrinkage = rgrid[i, 1], 
                   n.trees = rgrid[i, 2], 
                   interaction.depth = rgrid[i, 3],
                   n.cores = 3,
                   verbose = FALSE,
                   data = train
                   )

      # Predicting on the val set
phat &lt;- predict(model, n.trees = rgrid[i, 2], newdata = val, type = &quot;response&quot;)

  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, val$y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc_tuning[p] &lt;- auc_ROCR@y.values[[1]]
      
    }
   V_auc_mean[i] &lt;- mean(auc_tuning) # for each hyperparameter we are taking the mean of their runs
  
     BI &lt;- which.max(V_auc_mean) #best hyperparameter index
  best_AUC &lt;- V_auc_mean[BI]
  best_params &lt;- rgrid[BI, ]
  best_params &lt;- as.matrix(best_params)
  best_params &lt;- t(best_params)
  
  # store the hyperparameters and there average AUC
  opt[j, 1] &lt;- best_params[1, 1]
  opt[j, 2] &lt;- best_params[1, 2]
  opt[j, 3] &lt;- best_params[1, 3]
  opt[j, 4] &lt;- max(V_auc_mean)
  }
  
  # model for the final test witht he model data
   model &lt;- gbm(y ~ ., 
                   distribution = &quot;adaboost&quot;, 
                   bag.fraction = 1,
                   shrinkage = opt[j, 1], 
                   n.trees = opt[j, 2], 
                   interaction.depth = opt[j, 3],
                   n.cores = 3,
                   verbose = FALSE,
                   data = md
                   )

      # Predicting on the test set
 phat &lt;- predict(model, newdata = test, n.trees = opt[j, 2], type = &quot;response&quot;)

  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, test$y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc_t &lt;- auc_ROCR@y.values[[1]]  
    
 # store the test score
  opt[j, 5] &lt;- auc_t
}</code></pre>
<p>take the top x percentage of test scores in opt and put them and
there hyper parameters into win</p>
<pre class="r"><code>th &lt;- quantile(opt[ ,5], probs = 0.50)
ind_opt &lt;- which(opt[ ,5] &gt;= th)

win &lt;- opt[ind_opt, ]
win</code></pre>
<pre><code>##       shrinkage  n.trees interaction.depth   val_AUC  Test_AUC
##  [1,]     0.040      200                 3 0.8841965 0.8922617
##  [2,]     0.080      300                 4 0.8861789 0.8900626
##  [3,]     0.185      100                 6 0.8748713 0.9266304
##  [4,]     0.005     1800                 4 0.8908478 0.8819872
##  [5,]     0.015     1100                 4 0.8868271 0.8973910
##  [6,]     0.075      100                 2 0.8928636 0.8873822
##  [7,]     0.010     1800                 4 0.8798745 0.9076493
##  [8,]     0.005     1200                 5 0.8811887 0.8995389
##  [9,]     0.015     2000                 2 0.8883971 0.8870224
## [10,]     0.090      400                 3 0.8858022 0.8895806
## [11,]     0.035      500                 5 0.8847830 0.8861730
## [12,]     0.080      700                 3 0.8912566 0.8896761
## [13,]     0.060      200                 5 0.8847135 0.9071162
## [14,]     0.020      600                 4 0.8739894 0.9215752
## [15,]     0.005     1100                 3 0.8845291 0.9056653
## [16,]     0.045      500                 2 0.8863613 0.8984983
## [17,]     0.010     1100                 3 0.8861433 0.8921569
## [18,]     0.095      100                 2 0.8826145 0.8973910
## [19,]     0.075      500                 6 0.8833833 0.8932148
## [20,]     0.070      100                 4 0.8796643 0.9425560
## [21,]     0.005     1700                 4 0.8794056 0.9284647
## [22,]     0.030      200                 5 0.8884063 0.9033759
## [23,]     0.090      300                 3 0.8857359 0.8825911
## [24,]     0.105      400                 3 0.8748950 0.9079743
## [25,]     0.020      800                 5 0.8851473 0.8891216</code></pre>
<p>This uses bootstrapping for making data sets. we test each
hyperparameter on the same sets/splits</p>
<pre class="r"><code>library(gbm)
library(ROCR)

n &lt;- 400
w &lt;- nrow(win)

#collums are hyper param set
bs_results &lt;- matrix(0, nrow = n, ncol = w)



for (i in 1:n) {
  for (j in 1:w){
    auc &lt;- c()
  #cat(&quot;loops: &quot;, i, j, &quot;\r&quot;)
  
  # bootstrap the data
  ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE))
  md &lt;- data[ind, ]
  test &lt;- data[-ind, ]

  model &lt;- gbm(y ~ ., 
                   distribution = &quot;adaboost&quot;, 
                   bag.fraction = 1,
                   shrinkage = win[j, 1], 
                   n.trees = win[j, 2], 
                   interaction.depth = win[j, 3],
                   n.cores = 3,
                   verbose = FALSE,
                   data = md
                   )

  
  # Predicting on the test set
  phat &lt;- predict(model, test, n.trees = win[j, 2], type = &quot;response&quot;)
  
  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, test$y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc &lt;- auc_ROCR@y.values[[1]]
  
  # matrix with n rows and w collums. each collum is a hyper parameter
 bs_results[i, j] &lt;- auc 
}
}</code></pre>
<p>this takes the colmeans and finds the best hyperparameter set and
assigns it to bh. we also cbind the other results onto win just incase
we want to look at the other options.</p>
<pre class="r"><code>bh_ind &lt;- which.max(colMeans(bs_results))
bh &lt;- win[bh_ind, ]
mean(bs_results[,bh_ind])</code></pre>
<pre><code>## [1] 0.8786019</code></pre>
<pre class="r"><code>cm &lt;- as.vector(colMeans(bs_results))
win &lt;- cbind(win, av_ovr_n_runs=cm)</code></pre>
<p>This just does a final test on our selected winning
Hyperparameters</p>
<pre class="r"><code>library(gbm)
library(ROCR)

n &lt;- 1000

 auc &lt;- c()
for(i in 1:n){
 # bootstap
  ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE))
  md &lt;- data[ind, ]
  test &lt;- data[-ind, ]
# build the model on md
  model &lt;- gbm(y ~ ., 
                   distribution = &quot;adaboost&quot;, 
                   bag.fraction = 1,
                   shrinkage = bh[1], 
                   n.trees = bh[2], 
                   interaction.depth = bh[3],
                   n.cores = 3,
                   verbose = FALSE,
                   data = md
                   )

  
  # Predicting on the test set
  phat &lt;- predict(model, test, n.trees = bh[2], type = &quot;response&quot;)
  
  # Calculating the AUC
  pred_rocr &lt;- prediction(phat, test$y)
  auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;)
  auc[i] &lt;- auc_ROCR@y.values[[1]]
  }

mean(auc)</code></pre>
<pre><code>## [1] 0.8755775</code></pre>
<pre class="r"><code>sd(auc)</code></pre>
<pre><code>## [1] 0.01786618</code></pre>
<pre class="r"><code># plot auc and 95% CI
plot(auc, col=&quot;red&quot;)
abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3)</code></pre>
<p><img src="Titanic_Adaboost_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
