---
title: "XGB_2"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: readable
    highlight: tango
date: "2024-01-28"
navbar:
  title: "Simon Raymond"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
    - text: "Titanic XGBoosting"
      href: Titanic_XGBoosting.html
    - text: "Titanic Adaboost"
      href: Titanic_Adaboost.html
    - text: "Titanic Nnet"
      href: Titanic_Nnet.html
    - text: "Titanic_LM_CART_and_RF"
      href: Titanic_LM_CART_RF.html
    - text: "XGB_2"
      href: XGB_2.html
    - text: "Speeds"
      href: Speeds.html
name: SimonRaymond
---
# Intro and purpose

The basic idea of this is to run each rgrid hyperparam a bunch of times with bootstrapped test sets so that we can then just chose which one has the best auc. This is giving practical real results. to save time i do a round that tests the top 10 percent(based on auc) more thoroughly then when we tested each hyperparameter individually.

# Set-up

I have ran the set up in this the same as in the other xgboost method

```{r, echo = FALSE}
suppressPackageStartupMessages({
    library(plotly)
    library(readr)
    library(dplyr)
    library(forcats)
    library(stringr)
    library(tidyr)
    library(DataExplorer)
    library(randomForest)
    library(xgboost)
    library(shiny)
    library(GGally)
    library(ROCR)
    library(ggplot2)
    library(doParallel)
})

# Loading datasets
data <- read_csv("train.csv")

# Feature Engineering
data <- data %>%
  mutate(
    Ticket_Length = nchar(Ticket),
    Ticket_Is_Number = !grepl("\\D", Ticket),
    Title = str_extract(Name, "\\b[[:alpha:]]+\\."),
    Name_Length = nchar(Name),
    Cabin_Letter = ifelse(!is.na(Cabin), substr(Cabin, 1, 1), NA_character_),
    Has_Cabin = !is.na(Cabin)
  ) %>%
  add_count(Title) %>%
  mutate(
    Title = ifelse(n < 10, 'Other', as.character(Title)),
    Cabin_Letter = ifelse(Cabin_Letter %in% c('T', 'G'), 'Missing', as.character(Cabin_Letter)),
    Cabin_Letter = factor(Cabin_Letter)
  )

# Data Cleaning
data <- data %>%
  select(-Name, -Ticket, -Cabin) %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor) & !one_of("Age", "Embarked"), fct_explicit_na, na_level = "Missing")) %>%
  mutate(Embarked = replace(Embarked, is.na(Embarked), "S")) %>%
  mutate(Pclass = as.factor(Pclass))

# Imputing missing values in 'Age'
data_with_age <- data %>% filter(!is.na(Age))
data_missing_age <- data %>% filter(is.na(Age))

model_age <- randomForest(Age ~ . - PassengerId, data = data_with_age, na.action = na.omit)
predicted_ages <- predict(model_age, newdata = data_missing_age)
data_missing_age$Age <- predicted_ages

data <- rbind(data_with_age, data_missing_age) %>% arrange(PassengerId)

# Final preparation for the data
sdata <- data %>%
  mutate(across(where(is.numeric) & !all_of(c("Survived", "PassengerId")), scale)) %>%
  as_tibble()

# Prepare data for XGBOOST
sdata$Survived <- as.numeric(factor(sdata$Survived, labels = c(0, 1))) - 1
xs <- model.matrix(~ . - 1 - Survived - PassengerId, data = sdata)
y <- sdata$Survived

```

As a reminder here is the data
```{r}
str(data)
anyNA(data)
```

I one hot coded it into xs and y. I am not using a sparse matrix

# Running The model

## Functions

```{r}

run_xgb <- function(xs, y, grd, v) {
  # Setup parallel computing
  nc <- detectCores() - 1
  cl <- makeCluster(nc)
  registerDoParallel(cl)

  results <- foreach(g = 1:nrow(grd), .combine='rbind', .packages=c('xgboost', 'ROCR')) %dopar% {
    auc <- numeric(v) # AUC scores for validations
    for (k in 1:v) {
      
      # Bootstrap sampling for validation
      idx <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
      tr_x <- xs[idx, ]
      tr_y <- y[idx]
      vl_x <- xs[-idx, ]  
      vl_y <- y[-idx]

      # Model training for validation
      prms <- list(
        booster = "gbtree",
        objective = "binary:logistic",
        max_depth = grd[g, "max_depth"], 
        eta = grd[g, "eta"],
        subsample = grd[g, "subsample"],
        colsample_bytree = grd[g, "colsample_bytree"],
        gamma = grd[g, "gamma"],
        min_child_weight = grd[g, "min_child_weight"],
        alpha = grd[g, "alpha"],
        lambda = grd[g, "lambda"]
      )
      xgb_tr <- xgb.DMatrix(data = tr_x, label = tr_y) 
      xm <- xgb.train(params = prms, data = xgb_tr, nrounds = grd[g, "nrounds"], verbose = FALSE, nthread = 1)
      
      # AUC calculation for validation
      phat <- predict(xm, xgb.DMatrix(data = vl_x))
      pred <- prediction(phat, vl_y)
      auc[k] <- performance(pred, "auc")@y.values[[1]]
    }

    # AUC mean and params
    auc_mean <- mean(auc)
    c(grd[g, ], auc_mean)
  }

  # Stop the cluster
  stopCluster(cl)

  # Convert results to a tibble and set column names
  res <- as_tibble(results)
  names(res) <- c(names(grd), "AUC_Mean")

  res <- res %>%
  mutate(across(everything(), ~unlist(.)))
  
  return(res)
}


```


```{r}
test_top_hp <- function(hp, xs, y, t) {
 
  # Setup parallel computing
  nc <- detectCores() - 1
  cl <- makeCluster(nc)
  registerDoParallel(cl)
  
  # Testing loop
  ts_res <- foreach(h = 1:nrow(hp), .combine = 'rbind', .packages = c('xgboost', 'ROCR')) %dopar% {
    aucs <- numeric(t)
    for (i in 1:t) {
      # Bootstrap sampling for testing
      idx <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
      mdx <- xs[idx, ]
      mdy <- y[idx]
      tx <- xs[-idx, ]  
      ty <- y[-idx]

      # Parameters for the model
      prms <- list(
        booster = "gbtree",
        objective = "binary:logistic",
        max_depth = hp[h, "max_depth"], 
        eta = hp[h, "eta"],
        subsample = hp[h, "subsample"],
        colsample_bytree = hp[h, "colsample_bytree"],
        gamma = hp[h, "gamma"],
        min_child_weight = hp[h, "min_child_weight"],
        alpha = hp[h, "alpha"],
        lambda = hp[h, "lambda"],
        nrounds = hp[h, "nrounds"]
      )
      
      # Train and test the model
      dtr <- xgb.DMatrix(data = mdx, label = mdy)
      dte <- xgb.DMatrix(data = tx, label = ty)
      mdl <- xgb.train(params = prms, data = dtr, nrounds = prms$nrounds, verbose = 0, nthread = 1)
      
      # AUC calculation
      ph <- predict(mdl, dte)
      pr <- prediction(ph, ty)
      aucs[i] <- performance(pr, "auc")@y.values[[1]]
    }
    
    # Average AUC and combine with hyperparameters
    c(hp[h, ], mean_auc = mean(aucs))
  }
  
  # Stop the cluster
  stopCluster(cl)
  
  # Convert to tibble and unlist columns
  ts_tbl <- as_tibble(ts_res) %>% mutate(across(everything(), ~unlist(.)))

  return(ts_tbl)
}
```

```{r}

test_auc <- function(xs, y, best_hp, runs) {
  aucs <- numeric(runs)
  
  for (i in 1:runs) {
    # Bootstrap sampling for testing
    idx <- unique(sample(nrow(xs), nrow(xs), replace = TRUE))
    md_x <- xs[idx, ]
    md_y <- y[idx]
    test_x <- xs[-idx, ]
    test_y <- y[-idx]

    # Set parameters for the model
    params <- list(
      booster = "gbtree",
      objective = "binary:logistic",
      max_depth = best_hp$max_depth, 
      eta = best_hp$eta,
      subsample = best_hp$subsample,
      colsample_bytree = best_hp$colsample_bytree,
      gamma = best_hp$gamma,
      min_child_weight = best_hp$min_child_weight,
      alpha = best_hp$alpha,
      lambda = best_hp$lambda
    )

    # Train and test the model
    md <- xgb.DMatrix(data = md_x, label = md_y)
    dtest <- xgb.DMatrix(data = test_x, label = test_y)
    model <- xgb.train(params = params, data = md, nrounds = best_hp$nrounds, verbose = 0, nthread = 1)

    # Calculate AUC
    phat <- predict(model, dtest)
    pred <- prediction(phat, test_y)
    aucs[i] <- performance(pred, "auc")@y.values[[1]]
  }

  return(aucs)
}
```


## Model Running

Now lets actually run our algorithm

### Validating

```{r}
grid <- expand.grid(
  eta = seq(0.1, 0.1, by = 0), 
  max_depth = seq(16, 20, by = 4),
  min_child_weight = seq(1, 1, by = 1), 
  subsample = seq(0.8, 0.8, by = 0), 
  colsample_bytree = seq(1, 1, by = 0),
  lambda = seq(4, 10, by = 2), 
  alpha = seq(0, 3, by = 1), 
  gamma = seq(0, 5, by = 1), 
  nrounds = c(50, 100, 125, 150) 
)
```

```{r}
conf_lev <- .95
num_max <- 5 # Define number around the maximum
n <- log(1-conf_lev)/log(1-num_max/nrow(grid))
ind <- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE)
rgrid <- grid[ind, ]
```

```{r}
v <- 250

# Call the modified run_xgb function
vr <- run_xgb(xs, y, rgrid, v)

print(head(vr), width = Inf)

```

### Testing

```{r}
# Sort vr by AUC_Mean and select the top 25%
top_hp <- vr %>%
  arrange(desc(AUC_Mean)) %>%
  slice_head(prop = 0.1)

```


```{r}
t <- 1500
# Call test_top_hp with top hyperparameters, features (xs), targets (y), and number of tests (t)
ts_res <- test_top_hp(top_hp, xs, y, t)

print(head(ts_res), width = Inf)


```

# Final AUC pres

```{r}
best_hp <- ts_res %>%
  dplyr::arrange(desc(mean_auc)) %>%
  dplyr::slice(1)

print(best_hp, width = Inf)
```


```{r}
r <- 1000
auc <- test_auc(xs, y, best_hp, r)

mauc <- mean(auc)
mauc
sd(auc)
```

As we can see our results are much better even with a small grid

```{r}
plot(auc, col="red")
abline(a = mean(auc), b = 0, col = "blue", lwd = 2)
abline(a = mean(auc)-1.96*sd(auc), b = 0, col = "green", lwd = 3)
abline(a = mean(auc)+1.96*sd(auc), b = 0, col = "green", lwd = 3)
```


# Tune for best threshold

We are going to use jstat but you can pick another confusion table metric if you want 

```{r}


r <- 1000 
best_thresholds <- numeric(r) 
j_stats <- numeric(r) 

for(i in 1:r) {
    # Bootstrap sampling
    ind <- sample(nrow(xs), size = nrow(xs), replace = TRUE)
    train_xs <- xs[ind, ]
    test_xs <- xs[-ind, ]
    train_y <- y[ind]
    test_y <- y[-ind]
    
    # Create DMatrix objects
    dtrain <- xgb.DMatrix(data = train_xs, label = train_y)
    dtest <- xgb.DMatrix(data = test_xs, label = test_y)
    
    # Extract parameters for the model from best_hp
    params <- list(
        booster = "gbtree",
        objective = "binary:logistic",
        eta = best_hp$eta,
        max_depth = best_hp$max_depth,
        subsample = best_hp$subsample,
        colsample_bytree = best_hp$colsample_bytree,
        min_child_weight = best_hp$min_child_weight,
        gamma = best_hp$gamma,
        alpha = best_hp$alpha,
        lambda = best_hp$lambda
    )
    
    # Train the model
    xgbmdl <- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0)
    
    # Predictions
    phat <- predict(xgbmdl, dtest)
    
    # ROCR predictions object
    pred <- prediction(phat, test_y)
    
    # Calculate performance measures
    perf <- performance(pred, measure = "sens", x.measure = "spec")
    sensitivity <- slot(perf, "y.values")[[1]]
    specificity <- slot(perf, "x.values")[[1]]
    thresholds <- slot(perf, "alpha.values")[[1]]
    j_stat <- sensitivity + specificity - 1
    
    # Find the best threshold (maximizing J statistic)
    best_idx <- which.max(j_stat)
    best_thresholds[i] <- thresholds[best_idx]
    j_stats[i] <- j_stat[best_idx]
}

# Calculate and print the average of the best thresholds
avg_best_threshold <- mean(best_thresholds)
cat("Average Best Threshold:", avg_best_threshold, "\n")

# Plot the distribution of max J statistics for each iteration
hist(j_stats, col = "blue", xlab = "Max J Statistic", ylab = "Frequency",
     main = "Distribution of Max J Statistics Across Bootstrap Iterations")
abline(v = mean(j_stats), col = "red", lwd = 2) # Mean J Stat line

```
